{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "â€œlabel_prediction_vincent_thrift",
      "provenance": [],
      "collapsed_sections": [
        "B4yLzjne6Tm6",
        "NaSSY_mynUVw"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "59RK1lkXU9NH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3eb2d1f1-c609-47c4-88a6-7d53fabb95b6"
      },
      "source": [
        "# collect Google Drive to Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-Bnqn00VGfY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "41ff3e72-99b3-45a6-9fce-b1b299498d61"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67970XzEVCAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import transformers\n",
        "# per the setting of transformers, to use any of its NLP model\n",
        "# we need to have three things: that is BertTokenizer, BertModel, BertConfig\n",
        "from transformers import BertTokenizer, BertModel, BertConfig"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4socdVzVEHS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad8cfba7-66c8-4dbb-b599-fbf174bb8707"
      },
      "source": [
        "# first, let's see if we have GPU so that we could train our model in GPU\n",
        "# GPU is really at parallel computation\n",
        "from torch import cuda\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print(device)\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2L80RjhU9s_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create an instance of BERT model \n",
        "# note that it takes time to download the BERT model (~ 440M)\n",
        "# BERT model is big, because it has a lot of paramters. \n",
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKo8Z3XUYKJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creat an instance of BERT tokenizer\n",
        "# as you can tell, the tokenizer is pretty small, only 232k in size\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyhRDyuDUGwu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 959
        },
        "outputId": "81cea993-7a9d-48fd-c635-46ea2ce8ca97"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = 'stage_1_label_sample.csv'\n",
        "raw_review = pd.read_csv(url)\n",
        "raw_review"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>product_title</th>\n",
              "      <th>product_category</th>\n",
              "      <th>star_rating</th>\n",
              "      <th>helpful_votes</th>\n",
              "      <th>total_votes</th>\n",
              "      <th>vine</th>\n",
              "      <th>verified_purchase</th>\n",
              "      <th>review_headline</th>\n",
              "      <th>review_body</th>\n",
              "      <th>review_date</th>\n",
              "      <th>social connectedness</th>\n",
              "      <th>environment</th>\n",
              "      <th>self_sufficiency</th>\n",
              "      <th>transparency_authenticity</th>\n",
              "      <th>tradition</th>\n",
              "      <th>individuality</th>\n",
              "      <th>diversity_equality</th>\n",
              "      <th>privacy</th>\n",
              "      <th>status</th>\n",
              "      <th>thrift_value</th>\n",
              "      <th>innovation</th>\n",
              "      <th>fun_adventure</th>\n",
              "      <th>health</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Crane Usa EE-5301 Cool Mist 2.3-Gallon Humidifier</td>\n",
              "      <td>Health &amp; Personal Care</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>If only it still worked....</td>\n",
              "      <td>This product was great! for about a month, I l...</td>\n",
              "      <td>7/28/2012</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Medline Herringbone Cotton Dish Towels, 1 Dozen</td>\n",
              "      <td>Health &amp; Personal Care</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Three Stars</td>\n",
              "      <td>The material isn't great. Doesn't dry hardly a...</td>\n",
              "      <td>12/16/2014</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CytoSport Muscle Milk Snack Protein Bar</td>\n",
              "      <td>Health &amp; Personal Care</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>good</td>\n",
              "      <td>This product tastes good, they are a bit small...</td>\n",
              "      <td>4/24/2013</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Toothco for ProResults</td>\n",
              "      <td>Health &amp; Personal Care</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Excellent</td>\n",
              "      <td>Excellent</td>\n",
              "      <td>8/6/2015</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reynolds Consumer Products 1001090008031 Alumi...</td>\n",
              "      <td>Health &amp; Personal Care</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Worst foil ever...</td>\n",
              "      <td>This foil is so thin that if a mosquito were t...</td>\n",
              "      <td>11/7/2013</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1656</th>\n",
              "      <td>Rocky Men's Thermal 2pc Set Long John Underwearâ€¦</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Comfortable fit</td>\n",
              "      <td>Its a value for money purchase which basically...</td>\n",
              "      <td>11/24/2014</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1657</th>\n",
              "      <td>Style J Indigo Elegance Long Denim Skirt</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Great skirt</td>\n",
              "      <td>I really like this skirt. The front seam is a ...</td>\n",
              "      <td>3/9/2015</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1658</th>\n",
              "      <td>Next Level Apparel Women's Terry Racerback Tank</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Not worth it</td>\n",
              "      <td>Stiff fabric - not stretchy Tshirt material - ...</td>\n",
              "      <td>9/4/2014</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1659</th>\n",
              "      <td>Playtex Women's Side Smoothing Underwire Bra</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Fits well, kind funky fabric</td>\n",
              "      <td>This bra fits well enough, definitely true to ...</td>\n",
              "      <td>9/18/2013</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1660</th>\n",
              "      <td>Women's Combed Cotton Rib-Knit Thermal Vest</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Women's Combed Cotton Rib-Knit Thermal Vest</td>\n",
              "      <td>it is not like it looks in the image!&lt;br /&gt;i d...</td>\n",
              "      <td>11/23/2012</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1661 rows Ã— 23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          product_title  ... health\n",
              "0     Crane Usa EE-5301 Cool Mist 2.3-Gallon Humidifier  ...      0\n",
              "1       Medline Herringbone Cotton Dish Towels, 1 Dozen  ...      0\n",
              "2               CytoSport Muscle Milk Snack Protein Bar  ...      0\n",
              "3                                Toothco for ProResults  ...      0\n",
              "4     Reynolds Consumer Products 1001090008031 Alumi...  ...      0\n",
              "...                                                 ...  ...    ...\n",
              "1656   Rocky Men's Thermal 2pc Set Long John Underwearâ€¦  ...      0\n",
              "1657           Style J Indigo Elegance Long Denim Skirt  ...      0\n",
              "1658    Next Level Apparel Women's Terry Racerback Tank  ...      0\n",
              "1659       Playtex Women's Side Smoothing Underwire Bra  ...      0\n",
              "1660        Women's Combed Cotton Rib-Knit Thermal Vest  ...      0\n",
              "\n",
              "[1661 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bO_VL6_GV6gF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_review = pd.DataFrame(None)\n",
        "\n",
        "train_review[['text','label']] = raw_review[['review_body','thrift_value']]\n",
        "\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4F0PknWZqxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_review = train_review.dropna()"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajQpmwt2Xfds",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "332d9016-a891-4627-8e12-05246ed0d5dd"
      },
      "source": [
        "train_review.describe()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1660.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.798795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.401022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             label\n",
              "count  1660.000000\n",
              "mean      0.798795\n",
              "std       0.401022\n",
              "min       0.000000\n",
              "25%       1.000000\n",
              "50%       1.000000\n",
              "75%       1.000000\n",
              "max       1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "513Gaudcu56u",
        "colab_type": "text"
      },
      "source": [
        "# Define a Customized Dataset Class and Setup the Dataloader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "luKm7Mh6vZi4",
        "colab": {}
      },
      "source": [
        "# first let's define some key parameters we will use later\n",
        "# note this is a relatively large number, making the training process slow, \n",
        "# but it's necessary becuase a lot of reviews are long.\n",
        "max_length = 128\n",
        "# how many raw inputs we feed into the train and validation model at once\n",
        "train_batch = 32\n",
        "valid_batch = 32\n",
        "\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "APMXoM-b81B4",
        "colab": {}
      },
      "source": [
        "# then we need to import the libraries we need\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5zOBRz6rvwfh",
        "colab": {}
      },
      "source": [
        "class YelpDataset(Dataset):\n",
        "\n",
        "  # here we want to  create a customized dataset--YelpDataset--which could take a raw text as input\n",
        "  # and encode it in BERT's way with a tokenizer. \n",
        "  # The tokenized input will be then fed into a BERT Model in the NN, which we will create later\n",
        "\n",
        "  # __init__ defines some necessary attributes for any instance created\n",
        "  # like: the dataset with raw text reviews, what tokenizer we want to use for encoding, \n",
        "  # the max length of sentence we need to pad or trancate\n",
        "  def __init__(self, dataset, tokenizer, max_len):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = dataset\n",
        "    self.text = dataset.text\n",
        "    self.label = dataset.label\n",
        "    self.max_len = max_len\n",
        "  \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # __getitem__ take index as input, \n",
        "    # in general/without customizatin, it returns the sample with the index in the dataset\n",
        "    # further we could customize its functionality, letting it to apply some operations on the \n",
        "    # index-specified sample before return a value for us\n",
        "    # here we use index to locate a specific review text we want to pre-process\n",
        "    # text = str(self.text[index])\n",
        "    # text = \" \".join(text.split())\n",
        "    text = self.text.iloc[index]\n",
        "\n",
        "    # then, we put the text into a BERT encode_plus\n",
        "    # important debug tips: \n",
        "    # for encode_plus, to keep all the raw inputs in the same lenght\n",
        "    # we need to specify BOTH padding and trancation in addition to max_length\n",
        "    # the former for short sentences and the latter for long ones\n",
        "    # failing to do either one will result in uneven lengths of encoded inputs,\n",
        "    # which will create troubles for your dataloader in the nn training sessions\n",
        "    outputs = self.tokenizer.encode_plus(\n",
        "        text,\n",
        "        max_length = self.max_len,\n",
        "        padding = \"max_length\",\n",
        "        truncation = \"longest_first\",\n",
        "        # note that in some versions of transformer in you local machine, the code is \n",
        "        # pad_to_max_length = True,\n",
        "        # truncation_strategy = 'longest_first',\n",
        "        # we might need to change the argument name a little to fit different version of transformers\n",
        "        add_special_tokens = True,\n",
        "        return_tensors = 'pt'\n",
        "    )\n",
        "    # recall the BERT Tokenizer session, it takes raw text as input\n",
        "    # and return input_ids, attention_mask\n",
        "    input_ids = outputs['input_ids']\n",
        "    attention_mask = outputs['attention_mask']\n",
        "    # we then store those values and put them together with the label info in the sample\n",
        "    # as the return of the __getitem__ method\n",
        "    return {\n",
        "        'input_ids': torch.tensor(input_ids, dtype = torch.long),\n",
        "        'attention_mask': torch.tensor(attention_mask, dtype = torch.long),\n",
        "        'label': torch.tensor(self.label.iloc[index], dtype = torch.float) \n",
        "    }\n",
        "\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tZLP1R7vBuKL",
        "colab": {}
      },
      "source": [
        "# split the train and validate dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_raw, valid_raw = train_test_split(train_review, test_size = 0.1)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M05X1vZf8yAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "60727980-94f4-40b2-ee1c-96dac3e53608"
      },
      "source": [
        "train_raw.label.value_counts()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    1186\n",
              "0     308\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXy-DvBz4Xej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "e2f1625a-d41f-4f27-feb0-eb0b2e59c857"
      },
      "source": [
        "#extra_status_text_1 = pd.read_csv('status_augmented_3_5%.csv')\n",
        "#extra_status_text_1.info()\n",
        "\n",
        "#train_raw = pd.concat(([train_raw,extra_status_text_1]))\n",
        "#train_raw.drop_duplicates(inplace=True)\n",
        "train_raw.label.value_counts()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    1186\n",
              "0     308\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eRqVwGM5v2_d",
        "colab": {}
      },
      "source": [
        "# create instances of the YelpDataset for raw trianing and validate datasets\n",
        "# recall that tokenizer has be defined by: tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') in the BERT tokenizer session\n",
        "train_processed = YelpDataset(train_raw, tokenizer, max_length)\n",
        "valid_processed = YelpDataset(valid_raw, tokenizer, max_length)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eEl2Jkdksm4i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3fd60c80-b82d-4f52-b2bc-9e6a1ba1c222"
      },
      "source": [
        "# check the attributes and method of train_processed\n",
        "train_processed.__len__()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1494"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s7HaY0bQCTXw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "b3097361-bb3c-4434-8668-e45e904a145c"
      },
      "source": [
        "# test the customized Dataset instance\n",
        "print(train_processed.__getitem__(10))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[  101,  2122,  2763,  4995,  1005,  1056,  2428,  2204,  2005,  2189,\n",
            "          1012,  1045,  1005,  1049,  2025,  2019,  6739,  1010,  2021,  1045,\n",
            "          2064,  5791,  2425,  1996,  4489,  2013,  2152,  2203,  2132, 19093,\n",
            "          2013,  2122,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,\n",
            "          1028,  2174,  1010,  1045,  4952,  2000,  2831,  2265,  2557,  2013,\n",
            "          2026,  3526,  3042,  3262,  1010,  1998,  2005,  2008,  2027,  2147,\n",
            "          6581,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]]), 'label': tensor(1.)}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hvnlzcFRRcxw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "0577378b-25b5-4267-f1e5-8b6bb95798e4"
      },
      "source": [
        "# see the dimension of the pre-processed data\n",
        "print(train_processed.__getitem__(10)['input_ids'].shape)\n",
        "# we can use the squeeze() method to remove the axis of \"1\", a method we will use later\n",
        "print(train_processed.__getitem__(10)['input_ids'].squeeze().shape)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 128])\n",
            "torch.Size([128])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k4T7kvxJEpZX",
        "colab": {}
      },
      "source": [
        "# important, run it again to create a new dataset loader for every NN model you train\n",
        "# lastly, set up the dataloader that serves as a pipeline feeding pre-processed data into the neural network\n",
        "train_sampler = RandomSampler(train_processed)\n",
        "train_loader = DataLoader(train_processed, batch_size = train_batch, num_workers = 0)\n",
        "valid_sampler = SequentialSampler(valid_processed)\n",
        "valid_loader = DataLoader(valid_processed, batch_size = valid_batch, num_workers = 0)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gl3urqqxHeH9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e547b9e1-a96e-47f0-c12d-7fc0124087ca"
      },
      "source": [
        "# exploer the attributes of dataloder\n",
        "# length of dataloader = len(dataset)/batch size\n",
        "print(len(train_loader))\n",
        "print(len(valid_loader))\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4yLzjne6Tm6",
        "colab_type": "text"
      },
      "source": [
        "# Define a Customized Neural Network with the First Hidden Layer as NLP Encoding Layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hLuI0Q3ZF2C2"
      },
      "source": [
        "Up to now, we have defined the customized Dataset class that preprocess the raw input, encoding them into input_ids and attention_mask that BERT model needs. We also set up the Dataloader that feed the pre-processed data into the Neural Network we are creating now. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oGetmmKNafv5",
        "colab": {}
      },
      "source": [
        "# import the functions we need use in the Netwrok Model\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ld8Oqz5Jv43c",
        "colab": {}
      },
      "source": [
        "# the neural network we create is a class called  YelpBERT, which inherits\n",
        "# the attributes and structures of the torch.nn.Module\n",
        "\n",
        "# the __init__ functino defines the necessary hidden layers of the NN\n",
        "# and the forward function set up the computation graph: the real calculation procedures of the NN\n",
        "# see the class session 7's slides and recording for details \n",
        "\n",
        "class YelpBERT(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(YelpBERT, self).__init__()\n",
        "    # recall that, we have defined model as: model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    # see BERT Model's input and output: https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "    self.l1 = model\n",
        "\n",
        "    # with this layer a percentage of all neurons will be randomly turned-off to prevent overfitting\n",
        "    # since our training dataset might be small, we really cannot afford a large neural network\n",
        "    # the output of the BERT layer is a vector of 768 elements, if fully connected to next linear layer\n",
        "    # then the current layer would have 768 neurons, too much! \n",
        "    # using dropout mechanism, we can randomly turn off a percentage of the neuraon in the training process\n",
        "    # literally reduced the number of neuron in the layer\n",
        "    self.l2 = torch.nn.Dropout(0.4) \n",
        "\n",
        "    # if you want to have multiple fully connected linear layer, use this\n",
        "    # self.l2 = torch.nn.Linear(768, 10)\n",
        "    # but again, if your training dataset is not very large, we may only offard one linear layer\n",
        "    \n",
        "    # last layer\n",
        "    self.l3 = torch.nn.Linear(768, 1)\n",
        "\n",
        "\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    # first layer\n",
        "    # the first layer utilize the BERT model (call \"model\" in __init__) to transfer the input_ids into\n",
        "    # contextualized word embeddings --numerical vectors\n",
        "    # we can customize the output of this layer such as using the [CLS] token or the mean of all input tokens\n",
        "   \n",
        "    # if you want to use the BERT output of the last self-attention layer, use this:\n",
        "    last, pooler = self.l1(input_ids = input_ids, attention_mask = attention_mask)\n",
        "   \n",
        "    # if you want to use BERT output from OTHER self-attention layers,use this:\n",
        "    # (note that BERT base model has 12 hidden self-attention layers), \n",
        "    # last, pooler, all = self.l1(input_ids = input_ids, attention_mask = attention_mask, output_hidden_states = True)\n",
        "    \n",
        "    # second layer\n",
        "    # here we could use the mean value of tokens of all raw inputs as the embedding of the whole input text\n",
        "    # and feed it into the second layer, use this: \n",
        "    # output from last BERT self-attention layer:\n",
        "    # initial index \"0\" for mean value of [cls] token and all non-padded tokens\n",
        "    # initial index \"1\" for mean value of all non-padded tokens only\n",
        "    output = last[:, 0 : attention_mask.sum(), :].mean(dim = 1)\n",
        "    # BERT output from other hidden self-attention layer:\n",
        "    # output = all[11][:, 0 : attention_mask.sum(), :].mean(dim = 1)\n",
        "    \n",
        "    # or use the [CLS] token of last layer\n",
        "    # output = last[:, 0, :]\n",
        "\n",
        "    # output from BERT model now be fed into a relu activation funcation adn\n",
        "    # the second layer of the Neural Network\n",
        "    output = F.relu(output.squeeze())\n",
        "    output = F.relu(self.l2(output))\n",
        "\n",
        "    # why squeeze(): to make the dimension of input-output across layers consistent\n",
        "    # e.g., the output of layer 1--self.l1, is in the shape of (1, 768),\n",
        "    # we use .squeeze() to make it in a shape of (768) only, \n",
        "    # because the later layer--l2 adn l2--take (768) as input dimension not (1, 768)\n",
        "\n",
        "    # third layer\n",
        "    output = self.l3(output)\n",
        "\n",
        "    # last sigmoid layer to furhter transfer the single scalar of l3 into a probability\n",
        "    return torch.sigmoid(output)\n",
        "    # note that we can also customize the layers after the 1st one, \n",
        "    # making more layers (i.e., a deeper NN) and see if it perform betters "
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m8yf6W_iSdCJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "db92d4b8-5c06-4a69-acfb-9d402ba6db55"
      },
      "source": [
        "# test the model step-by-step\n",
        "# compare the output dimension to your expectation\n",
        "# https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "# testing each layer in the NN is very important\n",
        "# we should pay close attention the the dimensions of the inputs and outputs of each layer \n",
        "# use .squeeze() to remove uncessary axis whose length is 1 to make the dimensions consistent. like \"output = pooler.squeeze()\" in the NN above\n",
        "l1 = model\n",
        "l1.to(device)\n",
        "input_ids = train_processed.__getitem__(10)['input_ids'].to(device)\n",
        "attention_mask = train_processed.__getitem__(10)['attention_mask'].to(device)\n",
        "last, pooler = l1(input_ids = input_ids, attention_mask = attention_mask)\n",
        "print(last.shape)\n",
        "print(last.squeeze().shape)\n",
        "print(pooler.shape)\n",
        "print(pooler.squeeze().shape)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 128, 768])\n",
            "torch.Size([128, 768])\n",
            "torch.Size([1, 768])\n",
            "torch.Size([768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zuR1CGxHpNDt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "e94bfe0f-9839-4937-9c4b-31ebe7081af4"
      },
      "source": [
        "# similarly, test if the dimensions of input and output \n",
        "output = last[:, 1 : attention_mask.sum(), :].mean(dim = 1)\n",
        "output = output.squeeze()\n",
        "output.shape\n",
        "l2 = torch.nn.Dropout(0.4)   \n",
        "l2.to(device)\n",
        "l3 = torch.nn.Linear(768, 1)\n",
        "l3.to(device)\n",
        "output.to(device)\n",
        "l3(l2(torch.tensor(output)))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.3116], device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NaSSY_mynUVw"
      },
      "source": [
        "# Create a BERT + NN Model as an Instance of the Customized Model Classs Defined Above and Setup the Loss Function and Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aFDDnyD6wlD4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d03727e0-5198-4943-f594-79a598bad792"
      },
      "source": [
        "# create an instance of the YelpBERT model\n",
        "# remember to recreat a instance of the Model Class after you modified the YelpBERT class\n",
        "model_yelp = YelpBERT()\n",
        "model_yelp.to(device)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "YelpBERT(\n",
              "  (l1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (l2): Dropout(p=0.4, inplace=False)\n",
              "  (l3): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2BnImCNBLxG_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "076340e8-49a0-4f06-d324-61eee5743dff"
      },
      "source": [
        "# test the model_yelp\n",
        "# recall that the train_processed is a special class that could pre-process the raw input words into input_ids\n",
        "# and attentino_mask using its method __getitem__ method\n",
        "# we then use this method to get the input_ids and attention_mask that we need to feed in the YelpBERT() model\n",
        "\n",
        "test_output = model_yelp(input_ids, attention_mask)\n",
        "print(len(test_output))\n",
        "print(test_output)\n",
        "# note that the output is a scalar, becasue ofthe last hidden layer in the NN, self.l3 = torch.nn.Linear(768, 1)\n",
        "# the scalar then can be put into a softmax for prediction \n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "tensor([0.5382], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7QmZRjGkPVAI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "df4b3914-1a4d-4733-a721-565cdb4b273d"
      },
      "source": [
        "print(input_ids.shape)\n",
        "print(attention_mask.shape)\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 128])\n",
            "torch.Size([1, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nrrzk6ukepKO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a4c68658-c790-4288-dd77-b2488b1827fb"
      },
      "source": [
        "# check out the model parameters\n",
        "# note that the first BERT layer, the word embeddings are in the shape of (30522, 768)\n",
        "# and we can tell that the first 12 layers of our model are normal BERT layers, \n",
        "# the last two are what we customized-- a dropout layer and the linear layer\n",
        "model_yelp.parameters"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of YelpBERT(\n",
              "  (l1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (l2): Dropout(p=0.4, inplace=False)\n",
              "  (l3): Linear(in_features=768, out_features=1, bias=True)\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4enOR93aXnCG"
      },
      "source": [
        "Up to now, we have the Dataset (pre-process/encode the raw text inputs, making them into input_ids that BERT model takes as input), the Dataloader (feed the processed data into the NN in batchs), the neural network (with the first layer as a BERT model, the transfer the encoded input_ids into word embeddings, and later layers just work on those embeddings/numerical vectors as normal neural network). \n",
        "\n",
        "And finally, we could set up our training loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iZpo8C-zc-B3"
      },
      "source": [
        "First we start with: the loss function (e.g., crossentropy loss or mean squared error loss) and the optimize schedule (some thing about learning rate, adaptive learning rate, see class 7)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7TRC6Dbzc8oZ",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q4QtC037dzxO",
        "colab": {}
      },
      "source": [
        "# criterion is the loss function we use\n",
        "# here we use Binary CrossEntropy Loss, you can try others see the performance difference\n",
        "# https://pytorch.org/docs/master/generated/torch.nn.BCELoss.html\n",
        "criterion = nn.BCELoss()\n",
        "# note that loss function in pytorch framework usually take the pair of (prediction, ground_truth) as input\n",
        "# and give the loss value as output"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d0MvxRbleYiO",
        "colab": {}
      },
      "source": [
        "# optimizer is our optimize strategy, here we use stachastic gradient descending as the approch \n",
        "# to update our model parameters. \n",
        "# tips: previously I trained multiple models but with learning rate = 1e-05, 3e-05, \n",
        "# after 4 epochs the model performance almost doesn't change\n",
        "# thus now I use 10e-05, it seems the performance improves faster\n",
        "learning_rate = 10e-05\n",
        "# SGD is a common optimizer, but let's use Adam here, AdamW is a optimizer developed by Huggingface using Adam's mechanism\n",
        "# optimizer = optim.SGD(model_yelp.parameters(), lr = learning_rate, momentum = 0.9)\n",
        "# http://deeplearning.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/\n",
        "# you can try out other optimization method and see performance difference"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "njBahjNyjByc",
        "colab": {}
      },
      "source": [
        "# set the epoch\n",
        "epochs = 5\n",
        "# epochs means how many rounds each training sample will be fed into the NN.\n",
        "# next, we need to supply our model \"model_yelp\" to the GPU, so it can be run on GPU\n",
        "# model_yelp.to(device)\n",
        "# since the GPU has a lot of cores, it takes some time to supply to model to the GPU\n"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEUQBFhMhsHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try another optimizer\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "optimizer = AdamW(model_yelp.parameters(),lr = learning_rate,eps = 1e-8)\n",
        "                  \n",
        "total_steps = len(train_loader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler that update the learning rate gradully, this scheduler is with AdamW\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WUy2sgBBgz50",
        "colab": {}
      },
      "source": [
        "# lastly, let's define a helper function we can use for calculating the prediction accuracy\n",
        "import numpy as np\n",
        "# the idea of the funtion is that a vector of predicted probablity of being good review, \n",
        "# which is the output of the sigmoid/last layer of the neural network, is compared with the true label\n",
        "# is the predicted probability >= 0.5, we assign 1, otherwise we assign 0, we use np.around() achieve this\n",
        "def pred_accuracy (prediction, label):\n",
        "  pred_flat = np.around(prediction).flatten()\n",
        "  label_flat = label.flatten()\n",
        "  return np.sum(pred_flat == label_flat) / len(label_flat)\n"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2WFSgu6_1VL",
        "colab_type": "text"
      },
      "source": [
        "# Train the BERT + NN Model and Evaluate the Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DFPc5CRqiESW",
        "colab": {}
      },
      "source": [
        "# set the random set the same, making the results reproducible\n",
        "import random\n",
        "seed_val = 49\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_SCqZW_Dn6x5"
      },
      "source": [
        "OK, finally, we strat to train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpYKGk-NeCUZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "489c0efa-b050-4d10-9a3a-8b2e55390ea9"
      },
      "source": [
        "# create an instance of the YelpBERT model\n",
        "model_yelp = YelpBERT()\n",
        "model_yelp.to(device)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "YelpBERT(\n",
              "  (l1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (l2): Dropout(p=0.4, inplace=False)\n",
              "  (l3): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a417dCiz3VWS",
        "colab": {}
      },
      "source": [
        "# set up the directory for storing the trained model\n",
        "import os\n",
        "# save to Google Drive\n",
        "dir = \"/content/drive/My Drive\"\n",
        "# save to local file\n",
        "# dir = \"E://OneDrive - lmu.edu//Python Projects//BERT and ML\"\n"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H0gMiqrmix5g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "19adba4e-fbfe-4404-dad9-5e9379dfafc7"
      },
      "source": [
        "# Very Important: Each Time When You Train a Neural Network Again, \n",
        "# Plase recreate the nural network, the dataloader, and the optimizer and its scheduler.\n",
        "\n",
        "\n",
        "# train the model \n",
        "# store the total loss and accuracy values of each epoch\n",
        "training_stats = []\n",
        "\n",
        "# the epoch loop, number of rounds specified by epochs\n",
        "for epoch_i in range(0, epochs):\n",
        "  print('======== Epoch {:} / {:} =========='.format(epoch_i + 1, epochs))\n",
        "  # zero the values of total loss and accuracy of the epoch\n",
        "  total_loss = 0\n",
        "  total_accuracy = 0\n",
        "  # in the training stage, set the model into train mode\n",
        "  model_yelp.train()\n",
        "\n",
        "  # the training step loop\n",
        "  # recall that train_loader feed data into the NN in batchs to save memory and improve efficiency\n",
        "  # then the # of total steps is about (# of samples/batch size)\n",
        "  print(\"training\")\n",
        "  for step, batch in enumerate(train_loader, 0):\n",
        "    # feed the data into GPU using .to(device) method\n",
        "    # note that the input_ids for one raw input is in the shape of (1, max_length)\n",
        "    # after be processed into batch, the input_ids become (batch_size, 1, max_length)\n",
        "    # however, BERT only takes input_ids in the shape of (batch_size, max_length),\n",
        "    # (see here: https://huggingface.co/transformers/model_doc/bert.html#bertmodel)\n",
        "    # so we need to do \"batch['input_ids'].squeeze()\" inestead of \"batch['input_ids']\"\n",
        "    # to remove the unnecessy axis of \"1\". \n",
        "    # same operation for attention_mask\n",
        "    input_ids = batch['input_ids'].squeeze().to(device, dtype = torch.long)\n",
        "    attention_mask = batch['attention_mask'].squeeze().to(device, dtype = torch.long)\n",
        "    label = batch['label'].to(device, dtype = torch.float)\n",
        "\n",
        "    # at each step, before the NN does the feed forward, let's set the gradient to 0\n",
        "    # as pytorch nn.Module automatically cumulates gradient from previous rounds\n",
        "    # this is good for RNN training, but not necessy for us here. Thus, we turn it off.   \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # for each raw input, the feed forward calculation give us two scalar, \n",
        "    # representing the score/probability of the sample being 0/bad review class or 1/good review class  \n",
        "    # note that since we feed the inputs/raw samples in batch, \n",
        "    # the prediciton should be in the shape of (batch_size, # of classes)\n",
        "    prediction = model_yelp(input_ids, attention_mask)\n",
        "    prediction = prediction.squeeze()\n",
        "\n",
        "    # criterion is defined as a cross entropy loss function\n",
        "    # it takes (prediction, ground truth) as input arugments\n",
        "    # the former should be in the shape of (batch_size, # of classes), \n",
        "    # the latter should be in the sahpe of (batch_szie, 1), \"1\" dimension records the true class id of the input\n",
        "    # https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html   \n",
        "    loss = criterion(prediction, label)\n",
        "\n",
        "    # with the loss, we can do back propagation to calculate the gradient\n",
        "    # very easy, just one line of code  \n",
        "    loss.backward()\n",
        "\n",
        "    # with the gradient, we can update the model paramters. \n",
        "    # recall how we define the optimizer in the above cell  \n",
        "    optimizer.step()\n",
        "    \n",
        "    # update the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # to keep tracking on the model performance, we accumulate the total loss in every epoch  \n",
        "    total_loss += loss.item()\n",
        "      \n",
        "    # for every 10 steps, we print out the epoce # and loss\n",
        "    # again, the max_length of raw text input is 256, relatively long than usual, thus it will take longer to train. \n",
        "    if step%10 == 0 and step != 0:\n",
        "      print(f'Epoch:{epoch_i + 1}, Total_Loss:{total_loss}, Average_Loss:{total_loss/step}')\n",
        "  \n",
        "  # calcualte and store the average training loss of each batch in the current epoch\n",
        "  avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "  \n",
        "\n",
        "  # now let's set up the validation loop, meaning the trained model above will be used to evaluate the sample in validation dataset\n",
        "  # note that this validation loop is in the same \"indent\" level as the training loop, and they both under the epoches loop\n",
        "  print(\"validating\")\n",
        "  # set the model now in the evaluation mode\n",
        "  model_yelp.eval()\n",
        "  # zero the values of total loss and accuracy\n",
        "  total_loss = 0\n",
        "  #total_accuracy = 0\n",
        "\n",
        "  # validation loop\n",
        "  for step, batch in enumerate(valid_loader, 0):\n",
        "    input_ids = batch['input_ids'].squeeze().to(device, dtype = torch.long)\n",
        "    attention_mask = batch['attention_mask'].squeeze().to(device, dtype = torch.long)\n",
        "    label = batch['label'].to(device, dtype = torch.float)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction = model_yelp(input_ids, attention_mask)\n",
        "        prediction = prediction.squeeze()\n",
        "        loss = criterion(prediction, label)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    if step%10 == 0 and step != 0:\n",
        "      print(f'Epoch:{epoch_i + 1}, Total_Loss:{total_loss}, Average_Loss:{total_loss/step}')\n",
        "\n",
        "  # calcualte and store the average training loss of each batch in the current epoch\n",
        "  avg_valid_loss = total_loss / len(valid_loader)\n",
        "\n",
        "  training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_valid_loss\n",
        "        }\n",
        "    )\n",
        "  \n",
        "\n",
        "\n",
        "  # lastly, at the end of each epoch, let's save the model for later use\n",
        "  # note that dir is defined before as \"./content/drive/My Drive\"  \n",
        "  torch.save(model_yelp.state_dict(), os.path.join(dir, 'exper-epoch-{}.pt'.format(epoch_i)))\n",
        "\n",
        "print(\"training complete!\")\n",
        "\n"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======== Epoch 1 / 5 ==========\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Total_Loss:5.539227485656738, Average_Loss:0.5539227485656738\n",
            "Epoch:1, Total_Loss:10.457455068826675, Average_Loss:0.5228727534413338\n",
            "Epoch:1, Total_Loss:14.550578191876411, Average_Loss:0.48501927306254705\n",
            "Epoch:1, Total_Loss:20.028997138142586, Average_Loss:0.5007249284535646\n",
            "validating\n",
            "======== Epoch 2 / 5 ==========\n",
            "training\n",
            "Epoch:2, Total_Loss:4.831211805343628, Average_Loss:0.4831211805343628\n",
            "Epoch:2, Total_Loss:9.005742728710175, Average_Loss:0.4502871364355087\n",
            "Epoch:2, Total_Loss:12.540767699480057, Average_Loss:0.41802558998266853\n",
            "Epoch:2, Total_Loss:17.611664950847626, Average_Loss:0.44029162377119063\n",
            "validating\n",
            "======== Epoch 3 / 5 ==========\n",
            "training\n",
            "Epoch:3, Total_Loss:3.8667531609535217, Average_Loss:0.38667531609535216\n",
            "Epoch:3, Total_Loss:6.743477404117584, Average_Loss:0.3371738702058792\n",
            "Epoch:3, Total_Loss:9.679795905947685, Average_Loss:0.3226598635315895\n",
            "Epoch:3, Total_Loss:12.9812650680542, Average_Loss:0.324531626701355\n",
            "validating\n",
            "======== Epoch 4 / 5 ==========\n",
            "training\n",
            "Epoch:4, Total_Loss:2.7189532220363617, Average_Loss:0.27189532220363616\n",
            "Epoch:4, Total_Loss:5.00637748837471, Average_Loss:0.2503188744187355\n",
            "Epoch:4, Total_Loss:6.804497197270393, Average_Loss:0.22681657324234644\n",
            "Epoch:4, Total_Loss:9.101079680025578, Average_Loss:0.22752699200063944\n",
            "validating\n",
            "======== Epoch 5 / 5 ==========\n",
            "training\n",
            "Epoch:5, Total_Loss:1.287179920822382, Average_Loss:0.1287179920822382\n",
            "Epoch:5, Total_Loss:2.596425522118807, Average_Loss:0.12982127610594035\n",
            "Epoch:5, Total_Loss:3.5677383691072464, Average_Loss:0.11892461230357489\n",
            "Epoch:5, Total_Loss:5.067270711064339, Average_Loss:0.12668176777660847\n",
            "validating\n",
            "training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B5soA_VPd2Ry",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "95986701-ee95-4847-fb12-6a552fbb3c89"
      },
      "source": [
        "# plot the train statistics stored in training_stats\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set(style = \"darkgrid\")\n",
        "\n",
        "sns.set(font_scale = 1.5)\n",
        "plt.rcParams['figure.figsize'] = [12, 6]\n",
        "\n",
        "df_stats = pd.DataFrame(data = training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f8feae14cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfaFU4jMGbAC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "3532b84a-110b-4f66-d08a-e5a2a2d295e1"
      },
      "source": [
        "df_stats['Valid. Loss']"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "epoch\n",
              "1    0.427512\n",
              "2    0.328287\n",
              "3    0.366736\n",
              "4    0.327375\n",
              "5    0.365130\n",
              "Name: Valid. Loss, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9zXX5Pdvw9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make sure to save the model performance statistics and the associated model configuration. \n",
        "# that is save the df_stats to csv file with a file name, say experiment 1\n",
        "df_stats.to_csv(os.path.join(dir, 'label_experiment_4.csv'))\n",
        "# save the key hyper-parameters of this training experiment, say experiment 1\n",
        "label_experiment_4_config = {\n",
        "    \"epochs\": epochs,\n",
        "    \"train_batch_size\" : train_batch,\n",
        "    \"valid_batch_size\" : valid_batch,\n",
        "    \"initial_learning_rate\": learning_rate,\n",
        "     \"max_sentence_length\": max_length,\n",
        "     \"loss_fucntion\": criterion,\n",
        "     \"optimizer\": optimizer,\n",
        "     # you need to manually type-in the following info\n",
        "     \"BERT output\": \"mean value of [cls] embeddings of non-padded token from the second to the last layer\",\n",
        "     \"activation function\": \"relu\",\n",
        "     \"dropout rate of BERT output\": model_yelp.l2,\n",
        "     \"# of fully connected linear layer\": 1,\n",
        "     \"dataset\": \"Yelp Review Balanced\",\n",
        "     \"comment\": \"sees not as good as the outputs of last layer\"\n",
        "}\n"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odvnYYhghsH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the experiment configurate assocaited with this experiment\n",
        "# note that if you click the file icon (the third vertical one on the far left)\n",
        "# you will see the save files, double click on them, you can see them.\n",
        "import csv\n",
        "with open(os.path.join(dir, 'label_experiment_4_config.csv'), 'w') as csv_file:  \n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in label_experiment_4_config.items():\n",
        "       writer.writerow([key, value])"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WinIsU6zMpWt",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate the Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p9MQQ6yU9kLo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "299ba713-42f3-4ef0-ff11-9d1cf941eec7"
      },
      "source": [
        "# apply the trained model to the validation dataset\n",
        "# get the model predictions and compare the comparisons to the true labels\n",
        "model_yelp.eval()\n",
        "predictions, labels = [], []\n",
        "\n",
        "for step, batch in enumerate(valid_loader):\n",
        "  input_ids = batch['input_ids'].squeeze().to(device, dtype = torch.long)\n",
        "  attention_mask = batch['attention_mask'].squeeze().to(device, dtype = torch.long)\n",
        "  label = batch['label'].to('cpu').numpy()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    prediction = model_yelp(input_ids, attention_mask)\n",
        "\n",
        "  prediction = prediction.detach().cpu().numpy()\n",
        "  predictions.append(prediction)\n",
        "  labels.append(label)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tA38tbL6Qkv7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b85bf34-bd4a-4bc1-c7f5-89dac14167d3"
      },
      "source": [
        "# call the helper function-- pred_accuacy to compute the prediction accuracy in each batch\n",
        "ac = []\n",
        "for i in range(len(predictions)):\n",
        "  ac_i = pred_accuracy(predictions[i], labels[i])\n",
        "  ac.append(ac_i)\n",
        "ac"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.875, 0.96875, 0.75, 0.84375, 0.90625, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QaL7PHMQOvjZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "1eb71898-dc80-45de-ab4e-082c7fbcf306"
      },
      "source": [
        "# transfer the outcomes into np\n",
        "predictions = np.asarray(predictions)\n",
        "labels = np.asarray(labels)\n",
        "predictions[0]\n",
        "# note that now the outcomes are still stored in batches"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.97412574],\n",
              "       [0.9847233 ],\n",
              "       [0.98660463],\n",
              "       [0.9850299 ],\n",
              "       [0.9849054 ],\n",
              "       [0.98677886],\n",
              "       [0.986524  ],\n",
              "       [0.9827078 ],\n",
              "       [0.9856011 ],\n",
              "       [0.9828683 ],\n",
              "       [0.9631385 ],\n",
              "       [0.98386186],\n",
              "       [0.9780397 ],\n",
              "       [0.9867149 ],\n",
              "       [0.8489428 ],\n",
              "       [0.9860284 ],\n",
              "       [0.98111236],\n",
              "       [0.02016047],\n",
              "       [0.9858723 ],\n",
              "       [0.96207875],\n",
              "       [0.8906621 ],\n",
              "       [0.9842704 ],\n",
              "       [0.9673303 ],\n",
              "       [0.02819719],\n",
              "       [0.9016832 ],\n",
              "       [0.98631734],\n",
              "       [0.9660889 ],\n",
              "       [0.040386  ],\n",
              "       [0.9720443 ],\n",
              "       [0.97972095],\n",
              "       [0.02336227],\n",
              "       [0.98564595]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWiqZYkAhsID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2f60912-fb8e-4e82-e36d-3c1fce15ebb6"
      },
      "source": [
        "# convert predictions stored in allrevithe batches into a long vector\n",
        "pred = np.concatenate(predictions, axis=0 )\n",
        "pred = np.concatenate(pred, axis=0 )\n",
        "pred = pred.reshape(len(pred),1)\n",
        "print(pred.shape)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(166, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5kkRWD0hsIE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1561554f-0a15-4e64-8451-55f22e42b1dd"
      },
      "source": [
        "# convert the true labels batches into a long vector\n",
        "true_label = np.concatenate(labels, axis=0 )\n",
        "true_label = true_label.reshape(len(true_label), 1)\n",
        "print(true_label.shape)\n",
        "type(true_label)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(166, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXEasNQvhsIF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "a979ec92-3ecc-4cbf-ae56-889ddaf4c700"
      },
      "source": [
        "# put the predictions and labels into the same dataset\n",
        "df = np.concatenate([pred, true_label], axis = 1)\n",
        "df = pd.DataFrame(data=df, columns=[\"preds\", \"labels\"])\n",
        "df"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.974126</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.984723</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.986605</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.985030</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.984905</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>0.983622</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>0.979732</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>0.986397</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>0.978072</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>0.020557</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>166 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        preds  labels\n",
              "0    0.974126     1.0\n",
              "1    0.984723     1.0\n",
              "2    0.986605     1.0\n",
              "3    0.985030     1.0\n",
              "4    0.984905     0.0\n",
              "..        ...     ...\n",
              "161  0.983622     1.0\n",
              "162  0.979732     1.0\n",
              "163  0.986397     1.0\n",
              "164  0.978072     1.0\n",
              "165  0.020557     0.0\n",
              "\n",
              "[166 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVNeZmcJD0f_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "4fd77869-84aa-4cdb-9112-306c82866ce7"
      },
      "source": [
        "df.labels.value_counts()"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0    140\n",
              "0.0     26\n",
              "Name: labels, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pjqBh76onyd-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8fb8c3f2-a902-458f-aa43-509d671b98b9"
      },
      "source": [
        "# see the total prediction accuracy\n",
        "sum((df[\"preds\"]>=0.5) == df[\"labels\"])"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "145"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r7T9I_yhsIJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "b292f6cc-b1d0-4058-9595-d36becd4e98a"
      },
      "source": [
        "# find the index of the review that has the lowest predicted probabilty(of being a positive review) in true_label == 1 group. \n",
        "df.loc[df.loc[df['labels'] == 1, :].idxmin()]"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.020160</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.974126</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       preds  labels\n",
              "17  0.020160     1.0\n",
              "0   0.974126     1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VurVKLthsIK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c6770980-532a-4ccd-dbf0-398e64ee4663"
      },
      "source": [
        "# see that review\n",
        "valid_raw.iloc[14,0]\n"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Question,<br /><br />I read all these complaints about the company and the software. Why does Amazon still let this company sell products through the Amazon Marketplace with so many angry customers? Also, the product KeyGrabber USB KeyLogger 8MB Black is a piece of junk and does not work. If and when it does work, it does not capture the data from the keyboard that you need. Many online banking sites somehow block this device from capturing anything. I checked with my bank, I was told because of the way the site is coded, the device will not work. Since IE9 does not save my username and password either, everything on the market for this industry is dead. With so much technology out, it is really amazing the United States has no more people / programmers / developers / computer engineers with any brains left.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9wrzCUiLhCd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "outputId": "c7f95f30-e039-4887-d575-1329e25e2ab6"
      },
      "source": [
        "# alternatively, for all the reviews that have true_label == 1, \n",
        "# let's sort their predicted probabilities\n",
        "df.loc[df['labels'] == 0, :].sort_values(by='preds',ascending = False)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>0.986649</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.985601</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.984905</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>0.982969</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>0.981484</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.978040</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>0.972644</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>0.969228</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>0.957225</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.935809</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.925406</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>0.909673</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>0.553141</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0.533201</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>0.079767</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.040386</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>0.039818</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>0.039685</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>0.030564</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.028197</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>0.025142</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.023362</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.021824</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>0.020557</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.019225</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>0.018511</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        preds  labels\n",
              "107  0.986649     0.0\n",
              "8    0.985601     0.0\n",
              "4    0.984905     0.0\n",
              "108  0.982969     0.0\n",
              "149  0.981484     0.0\n",
              "12   0.978040     0.0\n",
              "94   0.972644     0.0\n",
              "104  0.969228     0.0\n",
              "87   0.957225     0.0\n",
              "93   0.935809     0.0\n",
              "40   0.925406     0.0\n",
              "92   0.909673     0.0\n",
              "66   0.553141     0.0\n",
              "78   0.533201     0.0\n",
              "111  0.079767     0.0\n",
              "27   0.040386     0.0\n",
              "76   0.039818     0.0\n",
              "129  0.039685     0.0\n",
              "159  0.030564     0.0\n",
              "23   0.028197     0.0\n",
              "145  0.025142     0.0\n",
              "30   0.023362     0.0\n",
              "43   0.021824     0.0\n",
              "165  0.020557     0.0\n",
              "80   0.019225     0.0\n",
              "131  0.018511     0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9PDrrPNhsIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "c71a2ea8-dbb5-42a7-a68e-af569b8e767e"
      },
      "source": [
        "# alternatively, for all the reviews that have true_label == 1, \n",
        "# let's sort their predicted probabilities\n",
        "df.loc[df['labels'] == 1, :].sort_values('preds')"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.020160</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>0.020953</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.032658</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.039149</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.071702</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>0.986613</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>0.986619</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>0.986714</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.986715</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.986779</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>140 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        preds  labels\n",
              "17   0.020160     1.0\n",
              "86   0.020953     1.0\n",
              "72   0.032658     1.0\n",
              "97   0.039149     1.0\n",
              "99   0.071702     1.0\n",
              "..        ...     ...\n",
              "114  0.986613     1.0\n",
              "56   0.986619     1.0\n",
              "132  0.986714     1.0\n",
              "13   0.986715     1.0\n",
              "5    0.986779     1.0\n",
              "\n",
              "[140 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FZooOSNTGDl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "5e36281d-6f21-4090-f8a6-4f535daf76fd"
      },
      "source": [
        "df[df['labels']==1].preds.sort_values()[0:20]"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17     0.020160\n",
              "86     0.020953\n",
              "72     0.032658\n",
              "97     0.039149\n",
              "99     0.071702\n",
              "128    0.323646\n",
              "156    0.403950\n",
              "55     0.604213\n",
              "69     0.636072\n",
              "138    0.636958\n",
              "146    0.652333\n",
              "35     0.710022\n",
              "41     0.794151\n",
              "151    0.803047\n",
              "118    0.807788\n",
              "14     0.848943\n",
              "79     0.849551\n",
              "20     0.890662\n",
              "24     0.901683\n",
              "110    0.911995\n",
              "Name: preds, dtype: float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wslamubJhsIQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "c278cff5-2fd0-4074-ef8d-05d1eb7e20f6"
      },
      "source": [
        "# see the reviews\n",
        "valid_raw.iloc[48,0]"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This is a great shower unit, but I too was surprised that the key parts were plastic.The plastic fitting that attached the unit to the pipe in the wall felt like the threads stripped when I was tightening it. I added more teflon tape and tried again more gently, and so far it has not leaked. Be careful not to over-tighten the plastic fitting. The hose is much stiffer than my previous shower unit, so it is harder to get it to stay out of the way.Otherwise, the unit has performed well and is very attractive.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otzAxck1hsIR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "outputId": "7d8c632f-cd18-41f1-df3f-45d384a72781"
      },
      "source": [
        "# on the other way around, for all the reviews whose label == 0, \n",
        "# let's sort their predicted probablities in descending order\n",
        "df[df['labels'] == 0].sort_values('preds', ascending = False)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>0.986649</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.985601</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.984905</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>0.982969</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>0.981484</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.978040</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>0.972644</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>0.969228</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>0.957225</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.935809</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.925406</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>0.909673</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>0.553141</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0.533201</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>0.079767</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.040386</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>0.039818</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>0.039685</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>0.030564</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.028197</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>0.025142</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.023362</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.021824</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>0.020557</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.019225</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>0.018511</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        preds  labels\n",
              "107  0.986649     0.0\n",
              "8    0.985601     0.0\n",
              "4    0.984905     0.0\n",
              "108  0.982969     0.0\n",
              "149  0.981484     0.0\n",
              "12   0.978040     0.0\n",
              "94   0.972644     0.0\n",
              "104  0.969228     0.0\n",
              "87   0.957225     0.0\n",
              "93   0.935809     0.0\n",
              "40   0.925406     0.0\n",
              "92   0.909673     0.0\n",
              "66   0.553141     0.0\n",
              "78   0.533201     0.0\n",
              "111  0.079767     0.0\n",
              "27   0.040386     0.0\n",
              "76   0.039818     0.0\n",
              "129  0.039685     0.0\n",
              "159  0.030564     0.0\n",
              "23   0.028197     0.0\n",
              "145  0.025142     0.0\n",
              "30   0.023362     0.0\n",
              "43   0.021824     0.0\n",
              "165  0.020557     0.0\n",
              "80   0.019225     0.0\n",
              "131  0.018511     0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjDkJr_VhsIU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "674864aa-b62a-44c3-88b3-1629c1a7dc32"
      },
      "source": [
        "# see the reviews\n",
        "# after learning some examples, it seems that our model will give a high score as long as the food is good\n",
        "# even though the service is not. \n",
        "valid_raw.iloc[140,0]"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I was hesitant to buy Turbo this year after having left to buy TaxCut last year when Intuit had the arrogance to put spyware into their product and add a disturbing activation process. I had always been a Turbo user but was so incensed at last years shenanigans by Intuit that I switched to TaxCut (and would do so again, should they initiate the same nonsense as they did in 2002). In any case, TaxCut in not even remotely as user friendly or robust as Turbo and given the opportunity to have Turbo as it was in 2001 and prior I have gladly come back. After reading the reviews on Amazon I hesitated, but am glad I gave it a try. The installation (on a Win 98 machine)was flawless (except for a minor glitch of it crashing my system when updating the software the first time). Useability is as good as always, in my opinion, which as I've said is light years ahead of TaxCut. In light of the options available on the market, Turbo is the clear choice (unless you want to suffer with a lesser product like TaxCut -- which I will grudgingly do again if Intuit plays games like they did in 2002!)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ_jr3kYUFro",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "outputId": "8be28c50-2096-407f-de54-e9ac6d5980f3"
      },
      "source": [
        "df['pred_label']=0\n",
        "df['pred_label'][df['preds']>0.5]=1\n",
        "df"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>labels</th>\n",
              "      <th>pred_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.974126</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.984723</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.986605</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.985030</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.984905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>0.983622</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>0.979732</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>0.986397</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>0.978072</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>0.020557</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>166 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        preds  labels  pred_label\n",
              "0    0.974126     1.0           1\n",
              "1    0.984723     1.0           1\n",
              "2    0.986605     1.0           1\n",
              "3    0.985030     1.0           1\n",
              "4    0.984905     0.0           1\n",
              "..        ...     ...         ...\n",
              "161  0.983622     1.0           1\n",
              "162  0.979732     1.0           1\n",
              "163  0.986397     1.0           1\n",
              "164  0.978072     1.0           1\n",
              "165  0.020557     0.0           0\n",
              "\n",
              "[166 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOPxWMaZXfJ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "6a87e3f3-5362-41c4-a76b-58c6b34153fe"
      },
      "source": [
        "df['preds'].sort_values(ascending=False)[0:20]"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5      0.986779\n",
              "13     0.986715\n",
              "132    0.986714\n",
              "107    0.986649\n",
              "56     0.986619\n",
              "114    0.986613\n",
              "153    0.986605\n",
              "2      0.986605\n",
              "137    0.986577\n",
              "47     0.986554\n",
              "6      0.986524\n",
              "98     0.986501\n",
              "64     0.986481\n",
              "121    0.986447\n",
              "150    0.986417\n",
              "163    0.986397\n",
              "25     0.986317\n",
              "85     0.986227\n",
              "116    0.986214\n",
              "117    0.986184\n",
              "Name: preds, dtype: float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5dypfVnW66r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "f3cfcce4-2059-4d9e-d942-7882434030a4"
      },
      "source": [
        "df['pred_label']"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      1\n",
              "1      1\n",
              "2      1\n",
              "3      1\n",
              "4      1\n",
              "      ..\n",
              "161    1\n",
              "162    1\n",
              "163    1\n",
              "164    1\n",
              "165    0\n",
              "Name: pred_label, Length: 166, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHOjxRiYgdPw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7bbd11ea-cd92-444a-bb90-13bd71c6f3f6"
      },
      "source": [
        "len(df['labels']) == len(df['pred_label'])"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vTzk6lYUe8P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "outputId": "ee0f8e3d-9a18-4e0f-84b1-dc73cfd4503f"
      },
      "source": [
        "import sklearn\n",
        "\n",
        "#Confusion Matrix\n",
        "confusion_matrix_census=sklearn.metrics.confusion_matrix(df['labels'], df['pred_label'], labels=None, sample_weight=None, normalize=None)\n",
        "tn, fp, fn, tp = confusion_matrix_census.ravel()\n",
        "print(confusion_matrix_census)\n",
        "\n",
        "#Precision and Recall\n",
        "precision=tp/(tp+fp)\n",
        "recall=tp/(tp+fn)\n",
        "print('Precision is ',precision,' , Recall is ',recall)\n",
        "\n",
        "\n",
        "#f1 score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "f1_score_cen=sklearn.metrics.f1_score(df['labels'], df['pred_label'],average='binary')\n",
        "print('built-in f1 score is ',f1_score_cen)\n",
        "\n",
        "\n",
        "#Accuracy score and AUC score\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(df['labels'], df['pred_label'])\n",
        "print('Accuracy score is ',accuracy)\n",
        "\n",
        "fpr, tpr, threshold = metrics.roc_curve(df['labels'], df['preds'])\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "print('AUC score is ',roc_auc)\n",
        "\n",
        "#drae the graph\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 12  14]\n",
            " [  7 133]]\n",
            "Precision is  0.9047619047619048  , Recall is  0.95\n",
            "built-in f1 score is  0.9268292682926829\n",
            "Accuracy score is  0.8734939759036144\n",
            "AUC score is  0.8008241758241759\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHM8Kq6g6t3b",
        "colab_type": "text"
      },
      "source": [
        "# **Predict on the colleted data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcb3rngd94Q4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "979410d1-5bd7-480b-b619-cb822deb4591"
      },
      "source": [
        "all_review = pd.read_csv('all_reviews_v4.0.csv')\n",
        "\n",
        "new_review = pd.DataFrame(None)\n",
        "\n",
        "#take a small sample for now\n",
        "new_review = all_review\n",
        "\n",
        "new_review['label']= 0\n",
        "\n",
        "new_raw = new_review.dropna(subset=['text'])\n",
        "\n",
        "new_raw"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>index</th>\n",
              "      <th>asin</th>\n",
              "      <th>productTitle</th>\n",
              "      <th>countReviews</th>\n",
              "      <th>date</th>\n",
              "      <th>imageUrlList</th>\n",
              "      <th>numberOfHelpful</th>\n",
              "      <th>parentReviewId</th>\n",
              "      <th>rating</th>\n",
              "      <th>reviewId</th>\n",
              "      <th>text</th>\n",
              "      <th>title</th>\n",
              "      <th>username</th>\n",
              "      <th>category</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>B07R7DSBVH</td>\n",
              "      <td>Columbia Boys' Big Glennaker Rain Jacket, Wate...</td>\n",
              "      <td>1571</td>\n",
              "      <td>7/16/2020</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>R385E1LP3AE40R</td>\n",
              "      <td>This Columbia jacket is a pretty good quality ...</td>\n",
              "      <td>Pretty good rain jacket</td>\n",
              "      <td>June B Furr</td>\n",
              "      <td>apparel</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>B07R7DSBVH</td>\n",
              "      <td>Columbia Boys' Big Glennaker Rain Jacket, Wate...</td>\n",
              "      <td>1571</td>\n",
              "      <td>7/15/2020</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>R36I6DUHDK4HIB</td>\n",
              "      <td>Just as expected! Perfect rain jacket for todd...</td>\n",
              "      <td>Toddler approved</td>\n",
              "      <td>Meeko0924</td>\n",
              "      <td>apparel</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>B07R7DSBVH</td>\n",
              "      <td>Columbia Boys' Big Glennaker Rain Jacket, Wate...</td>\n",
              "      <td>1571</td>\n",
              "      <td>7/14/2020</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>R4YTDBV1VENF6</td>\n",
              "      <td>I bought this for a Birthday gift and haven't ...</td>\n",
              "      <td>Birthday gift</td>\n",
              "      <td>Vera T.</td>\n",
              "      <td>apparel</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>B07R7DSBVH</td>\n",
              "      <td>Columbia Boys' Big Glennaker Rain Jacket, Wate...</td>\n",
              "      <td>1571</td>\n",
              "      <td>7/11/2020</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>RMQ5B577OTPJY</td>\n",
              "      <td>This is perfect for those fall and spring days...</td>\n",
              "      <td>Good jacket</td>\n",
              "      <td>RD</td>\n",
              "      <td>apparel</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>B07R7DSBVH</td>\n",
              "      <td>Columbia Boys' Big Glennaker Rain Jacket, Wate...</td>\n",
              "      <td>1571</td>\n",
              "      <td>7/10/2020</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>R12IYTDMAX94FD</td>\n",
              "      <td>good color</td>\n",
              "      <td>very good</td>\n",
              "      <td>ivy</td>\n",
              "      <td>apparel</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134961</th>\n",
              "      <td>135795</td>\n",
              "      <td>135796</td>\n",
              "      <td>B07FV9GW8S</td>\n",
              "      <td>Corel Paintshop Pro 2019 Ultimate - Photo with...</td>\n",
              "      <td>128</td>\n",
              "      <td>11/6/2018</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>R1IP5E8DS9YEP3</td>\n",
              "      <td>UPDATE 11/23/2018: I'm dropping the rating a s...</td>\n",
              "      <td>Excellent Value, Potential Alternative to Phot...</td>\n",
              "      <td>Amazon Customer</td>\n",
              "      <td>software</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134962</th>\n",
              "      <td>135796</td>\n",
              "      <td>135797</td>\n",
              "      <td>B07FV9GW8S</td>\n",
              "      <td>Corel Paintshop Pro 2019 Ultimate - Photo with...</td>\n",
              "      <td>128</td>\n",
              "      <td>11/5/2018</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>R2AVKQCZXW4IGN</td>\n",
              "      <td>PaintShop Pro 2019 comes in a box with a DVD, ...</td>\n",
              "      <td>A Quirky but Reasonable Application for Image ...</td>\n",
              "      <td>Jeffrey N. Fritz</td>\n",
              "      <td>software</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134963</th>\n",
              "      <td>135797</td>\n",
              "      <td>135798</td>\n",
              "      <td>B07FV9GW8S</td>\n",
              "      <td>Corel Paintshop Pro 2019 Ultimate - Photo with...</td>\n",
              "      <td>128</td>\n",
              "      <td>10/31/2018</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>R1YTIJIGJ4IMT7</td>\n",
              "      <td>I see by the reviews that a LOT of folks had p...</td>\n",
              "      <td>Some times you eat the bear and sometimes...</td>\n",
              "      <td>enubrius</td>\n",
              "      <td>software</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134964</th>\n",
              "      <td>135798</td>\n",
              "      <td>135799</td>\n",
              "      <td>B07FV9GW8S</td>\n",
              "      <td>Corel Paintshop Pro 2019 Ultimate - Photo with...</td>\n",
              "      <td>128</td>\n",
              "      <td>10/30/2018</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>R3FO4N6RUDI1EY</td>\n",
              "      <td>I had the disc and still experienced installat...</td>\n",
              "      <td>Paintshop Pro 2019 Ultimate - Photo with Multi...</td>\n",
              "      <td>new yorker</td>\n",
              "      <td>software</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134965</th>\n",
              "      <td>135799</td>\n",
              "      <td>135800</td>\n",
              "      <td>B07FV9GW8S</td>\n",
              "      <td>Corel Paintshop Pro 2019 Ultimate - Photo with...</td>\n",
              "      <td>128</td>\n",
              "      <td>10/2/2018</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>R15TC7MFA0X2K6</td>\n",
              "      <td>I hate this software. It is not user friendly....</td>\n",
              "      <td>Not user friendly</td>\n",
              "      <td>Reginald M. Partee</td>\n",
              "      <td>software</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>134966 rows Ã— 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0   index        asin  ...            username  category label\n",
              "0                0       1  B07R7DSBVH  ...         June B Furr   apparel     0\n",
              "1                1       2  B07R7DSBVH  ...           Meeko0924   apparel     0\n",
              "2                2       3  B07R7DSBVH  ...             Vera T.   apparel     0\n",
              "3                3       4  B07R7DSBVH  ...                  RD   apparel     0\n",
              "4                4       5  B07R7DSBVH  ...                 ivy   apparel     0\n",
              "...            ...     ...         ...  ...                 ...       ...   ...\n",
              "134961      135795  135796  B07FV9GW8S  ...     Amazon Customer  software     0\n",
              "134962      135796  135797  B07FV9GW8S  ...    Jeffrey N. Fritz  software     0\n",
              "134963      135797  135798  B07FV9GW8S  ...            enubrius  software     0\n",
              "134964      135798  135799  B07FV9GW8S  ...          new yorker  software     0\n",
              "134965      135799  135800  B07FV9GW8S  ...  Reginald M. Partee  software     0\n",
              "\n",
              "[134966 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVzqU0g_8SIQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "71901633-7862-4199-f4d5-58dac9788668"
      },
      "source": [
        "# apply the trained model to the NEW dataset\n",
        "# get the model predictions\n",
        "\n",
        "predictions_new, labels_new = [], []\n",
        "\n",
        "new_processed = YelpDataset(new_raw, tokenizer, max_length)\n",
        "\n",
        "new_loader = DataLoader(new_processed, batch_size = valid_batch, num_workers = 0)\n",
        "\n",
        "\n",
        "for step, batch in enumerate(new_loader):\n",
        "  input_ids_new = batch['input_ids'].squeeze().to(device, dtype = torch.long)\n",
        "  attention_mask_new = batch['attention_mask'].squeeze().to(device, dtype = torch.long)\n",
        "  label_new = batch['label'].to('cpu').numpy()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    new_prediction = model_yelp(input_ids_new, attention_mask_new)\n",
        "\n",
        "  new_prediction = new_prediction.detach().cpu().numpy()\n",
        "  predictions_new.append(new_prediction)\n",
        "  labels_new.append(label_new)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT1Zll_fDk79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo6E0bWB8R7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# call the helper function-- pred_accuacy to compute the prediction accuracy in each batch\n",
        "ac_new = []\n",
        "for i in range(len(predictions_new)):\n",
        "  ac_i = pred_accuracy(predictions_new[i], labels_new[i])\n",
        "  ac_new.append(ac_i)\n",
        "ac_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WFt0rGB28eHY",
        "colab": {}
      },
      "source": [
        "# transfer the outcomes into np\n",
        "predictions_new = np.asarray(predictions_new)\n",
        "labels_new = np.asarray(labels_new)\n",
        "predictions_new[0]\n",
        "# note that now the outcomes are still stored in batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "54_ue1sO8eHh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5f9e210-327d-416d-a2c1-678396f743c1"
      },
      "source": [
        "# convert predictions stored in the batches into a long vector\n",
        "pred_new = np.concatenate(predictions_new, axis=0 )\n",
        "pred_new = np.concatenate(pred_new, axis=0 )\n",
        "pred_new = pred_new.reshape(len(pred_new),1)\n",
        "print(pred_new.shape)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(134966, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ao0yHWq-8eHl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8438b626-6804-4df9-8df1-2b1d003f1998"
      },
      "source": [
        "# convert the true labels batches into a long vector\n",
        "true_label_new = np.concatenate(labels_new, axis=0 )\n",
        "true_label_new = true_label_new.reshape(len(true_label_new), 1)\n",
        "print(true_label_new.shape)\n",
        "type(true_label_new)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(134966, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7JBZel0I8eHq",
        "colab": {}
      },
      "source": [
        "# put the predictions and labels into the same dataset\n",
        "df_new = np.concatenate([pred_new, true_label_new], axis = 1)\n",
        "df_new = pd.DataFrame(data=df_new, columns=[\"preds\", \"labels\"])\n",
        "df_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l2LoQCIsAk-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "fd07036d-8e8e-4634-ee14-c94d1e0956ae"
      },
      "source": [
        "df_new"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.987033</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.009132</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.009905</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.009620</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.010506</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135795</th>\n",
              "      <td>0.983766</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135796</th>\n",
              "      <td>0.041913</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135797</th>\n",
              "      <td>0.013055</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135798</th>\n",
              "      <td>0.987761</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135799</th>\n",
              "      <td>0.010317</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>135800 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           preds  labels\n",
              "0       0.987033     0.0\n",
              "1       0.009132     0.0\n",
              "2       0.009905     0.0\n",
              "3       0.009620     0.0\n",
              "4       0.010506     0.0\n",
              "...          ...     ...\n",
              "135795  0.983766     0.0\n",
              "135796  0.041913     0.0\n",
              "135797  0.013055     0.0\n",
              "135798  0.987761     0.0\n",
              "135799  0.010317     0.0\n",
              "\n",
              "[135800 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YJUfCOiz8eHt",
        "colab": {}
      },
      "source": [
        "# see the total prediction accuracy\n",
        "#sum((df[\"preds\"]>=0.5) == df[\"labels\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M7JChA_X8eHw",
        "colab": {}
      },
      "source": [
        "# find the index of the review that has the lowest predicted probabilty(of being a positive review) in true_label == 1 group. \n",
        "df.loc[df.loc[df['labels'] == 0, :].idxmin()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s8bNtQYA8eHy",
        "colab": {}
      },
      "source": [
        "# see that review\n",
        "valid_raw.iloc[0,0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ufpqod3ZJWYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_new['label']= df_new['preds']>0.5\n",
        "df_new['label'] = df_new['label'].replace({True:1,False:0})\n",
        "df_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UTGv7FRO8eH1",
        "colab": {}
      },
      "source": [
        "# alternatively, for all the reviews that have true_label == 1, \n",
        "# let's sort their predicted probabilities\n",
        "df_new.loc[df_new['label'] == 0, :].sort_values('preds')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG_fByJ2KZMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_raw.iloc[3045,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHLoDTBMIAyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_new.loc[df_new['label'] == 1, :].sort_values('preds',ascending = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9QeRjmfFSfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_raw.iloc[14763,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqTJaGB9mjkI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68364591-4582-4fd1-b5cf-88528a044869"
      },
      "source": [
        "new_raw.reviewId.nunique()"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "134966"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk3QmNyUr1OB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vm4MZBWuKDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_raw['thrift_pred'] = df_new['preds']\n",
        "new_raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAC_kj9O7W_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_raw['helfulness'] = all_review['numberOfHelpful']\n",
        "\n",
        "new_raw['rating'] = all_review['rating']\n",
        "\n",
        "new_raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn_5_E4Vm71L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_raw = new_raw.set_index('reviewId')"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaRL4pMEnM7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g53WFqanw4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#join to the final dataset\n",
        "\n",
        "final = pd.read_csv('all_values_predicted.csv')"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4m9sYeJopVS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "bffea58d-981f-431a-ded6-e3044688ed30"
      },
      "source": [
        "final.thrift_pred.value_counts()"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.205241    656\n",
              "0.193674    489\n",
              "0.309197    398\n",
              "0.707394    313\n",
              "0.583936    275\n",
              "           ... \n",
              "0.182645      1\n",
              "0.088778      1\n",
              "0.954908      1\n",
              "0.989817      1\n",
              "0.814341      1\n",
              "Name: thrift_pred, Length: 90743, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ1bYT8joOtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final = final.set_index('reviewId')"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v88aMf-8ob6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nNBWtUBn9mh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final['thrift_pred'] = new_raw['thrift_pred']"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD6yOCtmo0zi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "023092d4-48a1-4757-8c7e-72d29d1017a7"
      },
      "source": [
        "final.thrift_pred.value_counts()"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.530585    684\n",
              "0.567157    505\n",
              "0.438953    408\n",
              "0.228029    329\n",
              "0.984015    286\n",
              "           ... \n",
              "0.689120      1\n",
              "0.981583      1\n",
              "0.029348      1\n",
              "0.939125      1\n",
              "0.023438      1\n",
              "Name: thrift_pred, Length: 90307, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYa_4Iz9o6yh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final.to_csv('all_values_predicted_v2.0.csv')"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoYh9BeZ7xpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_raw.to_csv('all_reviews_predicted_numbers_status.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1qjQJCkGhiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_raw.groupby('label').mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nespdd7jPmIU",
        "colab_type": "text"
      },
      "source": [
        "# **Save and Load a Trained Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsihC4eWPl8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check the tutorial here:\n",
        "# https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch\n",
        "\n",
        "# if you want to save the trained model for later evaluation (not for training)\n",
        "dir = \"/content/drive/My Drive/\" \n",
        "torch.save(model_yelp.state_dict(), os.path.join(dir, 'status_model.pt'))\n",
        "\n",
        "# later if you want to load the model and use it for evaluation (not trianing), do this: \n",
        "# the_model = TheModelClass(*args, **kwargs), in our case\n",
        "#new_model = YelpBERT()\n",
        "\n",
        "# and the_model.load_state_dict(torch.load(PATH)), note that path is specified above\n",
        "#new_model.load_state_dict(torch.load(path to the save model))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ9BBoshQClM",
        "colab_type": "text"
      },
      "source": [
        "# **BERT Enconding and Cluster Analysis**\n",
        "\n",
        "Up to now we have been focusing on using BERT to generate a contextulized embedding for the raw input reviews, then feeding the sentence embeddings into a neural network for classification. The classification analysis helps us to check if the review has some attributes (i.e.,positive or negative, manifiest some customer values or not) and to what extent the review has those attributes (i.e., the predicted probablity)\n",
        "\n",
        "Now, let's think about another use of the contextualized sentence embedding. We can do cluster analysis on those embeddings and see if there are some patterns in it. We can do it by locations (RMDS group's location-specific Yelp review analysis), product categories (RPA group's category-specific analysis), years (RPA group's time series analysi), and review type (e.g., positive vs. negative).\n",
        "\n",
        "By dong so, we can learn the representative reviews, keywords, opinions in each cluster, and by comparing the clusters derived from differnt locations, product categories, years, and review types. We can obtain a lot of insights!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niFAdIAPwxz3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "66f612ee-b887-4bf1-e10c-ac99473c4a3b"
      },
      "source": [
        "#don't predict, just import\n",
        "\n",
        "new_raw = pd.read_csv('all_reviews_predicted_status.csv')\n",
        "\n",
        "new_raw"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>date</th>\n",
              "      <th>category</th>\n",
              "      <th>label</th>\n",
              "      <th>helfulness</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>This Columbia jacket is a pretty good quality ...</td>\n",
              "      <td>7/16/2020</td>\n",
              "      <td>apparel</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Just as expected! Perfect rain jacket for todd...</td>\n",
              "      <td>7/15/2020</td>\n",
              "      <td>apparel</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>I bought this for a Birthday gift and haven't ...</td>\n",
              "      <td>7/14/2020</td>\n",
              "      <td>apparel</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>This is perfect for those fall and spring days...</td>\n",
              "      <td>7/11/2020</td>\n",
              "      <td>apparel</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>good color</td>\n",
              "      <td>7/10/2020</td>\n",
              "      <td>apparel</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135795</th>\n",
              "      <td>135795</td>\n",
              "      <td>UPDATE 11/23/2018: I'm dropping the rating a s...</td>\n",
              "      <td>11/6/2018</td>\n",
              "      <td>software</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135796</th>\n",
              "      <td>135796</td>\n",
              "      <td>PaintShop Pro 2019 comes in a box with a DVD, ...</td>\n",
              "      <td>11/5/2018</td>\n",
              "      <td>software</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135797</th>\n",
              "      <td>135797</td>\n",
              "      <td>I see by the reviews that a LOT of folks had p...</td>\n",
              "      <td>10/31/2018</td>\n",
              "      <td>software</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135798</th>\n",
              "      <td>135798</td>\n",
              "      <td>I had the disc and still experienced installat...</td>\n",
              "      <td>10/30/2018</td>\n",
              "      <td>software</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135799</th>\n",
              "      <td>135799</td>\n",
              "      <td>I hate this software. It is not user friendly....</td>\n",
              "      <td>10/2/2018</td>\n",
              "      <td>software</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>135800 rows Ã— 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0  ... rating\n",
              "0                0  ...      3\n",
              "1                1  ...      5\n",
              "2                2  ...      4\n",
              "3                3  ...      5\n",
              "4                4  ...      5\n",
              "...            ...  ...    ...\n",
              "135795      135795  ...      3\n",
              "135796      135796  ...      4\n",
              "135797      135797  ...      4\n",
              "135798      135798  ...      3\n",
              "135799      135799  ...      1\n",
              "\n",
              "[135800 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1FUY3ynz6-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to do cluster analysis, we first need to transfer the raw text review into embeddings\n",
        "# we can leverage our trianed NN model to do so. \n",
        "# Recall thate the first layer in our NN is a BERT model, whose job is exactly this. \n",
        "# one benefit of using our traind NN rather than a commnon instance of raw BERT Model \n",
        "# is that in our trained model, the BERT layer is also updated, its parameters \n",
        "# has be trained to understand our general context (e.g., Yelp or Amazon reviews)\n",
        "# and thus can encode our text better.\n",
        "\n",
        "# then, our job is to feed the raw text into our trained NN. \n",
        "# However, the final output of  our NN is a predicted probability\n",
        "# we thus need to extract the intermediate output of the NN, i.e., the output of its first layer, the BERT layer\n",
        "# here is a built-in function in Pytorch, call hook, to do the job-- extracting a NN's intermediate output\n",
        "\n",
        "# bascially, hook is just a mark attached to a specific NN layer, \n",
        "# telling the NN to store the intermediate output for later use\n",
        "# here, we store the intermediate output in inter_out\n",
        "\n",
        "inter_output = {}\n",
        "def get_inter_output(name):\n",
        "    def hook(model, input, output):\n",
        "        inter_output[name] = output[0].detach()\n",
        "    return hook\n",
        "\n",
        "handle = model_yelp.l1.register_forward_hook(get_inter_output('l1'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-CezcTEaxRV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a5981a7-0696-426e-b2d8-756ccf8abf7c"
      },
      "source": [
        "model_yelp(input_ids,attention_mask)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5312], device='cuda:0', grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgZB6201sODM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "handle.remove()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6RmIC0UpT58",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "259a91f3-af28-429b-dad5-5514cb98b2b0"
      },
      "source": [
        "len(inter_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WxlkzrjuQkP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70adb221-d259-46c1-ff2b-95901bf104df"
      },
      "source": [
        "inter_output['l1'].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0d9fKbLcrUD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "112ff046-894f-4be3-c847-74cb68b28877"
      },
      "source": [
        "model_test = YelpBERT()\n",
        "model_test.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "YelpBERT(\n",
              "  (l1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (l2): Dropout(p=0.3, inplace=False)\n",
              "  (l3): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RK3SijliWPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "handle.remove()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkSYOC0hvawQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93eaa59f-a40e-4ae1-9f63-62ffd9730f9b"
      },
      "source": [
        "inter_output[\"l1\"][:, 0 : attention_mask.sum(), :].mean(dim = 1).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc43idrKe6sR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9f6dd96-6a7e-4ef6-b2e6-55578f9fa1b9"
      },
      "source": [
        "model_yelp.named_modules()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.named_modules at 0x7f0233521af0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaQPZ0Y1EpCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first, let's convert the raw review texts into contextualized numerical embeddings \n",
        "# using encode_plus and BERT model\n",
        "\n",
        "def text_to_embedding(index, dataset, max_len):\n",
        "  # argument input: is the index to locate a specific text reivew\n",
        "  # argument dataset: is the dataset that containing text reviews\n",
        "  # argument max_len: is the max length of each review, for padding or truncating \n",
        "  \n",
        "  # extract the review text column in the dataset\n",
        "  # per my experience, extract a column and then using pd.iloc[index] on the column\n",
        "  # experience less bugs than pd.loc[index, 'column name']\n",
        "  reviews = dataset.text\n",
        "\n",
        "  # get the text review\n",
        "  text = reviews.iloc[index]\n",
        "  # transfer the text reivew into input_ids and attentino_mask that are required for BERT model\n",
        "  encoded_text = tokenizer.encode_plus(\n",
        "      text,\n",
        "      max_length = max_len,\n",
        "      # note that in some versions of transformer in you local machine, the code is \n",
        "      # pad_to_max_length = True,\n",
        "      # truncation_strategy = 'longest_first',\n",
        "      # we might need to change the argument name a little to fit different version of transformers\n",
        "      padding = \"max_length\",\n",
        "      truncation = \"longest_first\",\n",
        "      add_special_tokens = True,\n",
        "      return_tensors = \"pt\"\n",
        "  )\n",
        "  # make the input_ids and attention_mask into torch tensors, \n",
        "  # which will make the late use of BERT Model easier\n",
        "  # note that we use to(device) to deliever the data to GPU, \n",
        "  # so that they can be used in model_yelp, which is alread in GPU\n",
        "  input_ids = torch.tensor(encoded_text['input_ids'], dtype = torch.long).to(device)\n",
        "  attention_mask = torch.tensor(encoded_text['attention_mask'], dtype = torch.long).to(device)\n",
        "  \n",
        "  # transfer the input_ids and attention_mask into sentence embeddings by\n",
        "  # putting them into our trained NN model: mddel_yelp\n",
        "  model_yelp(input_ids, attention_mask)\n",
        " \n",
        "  \n",
        "  text_embedding = inter_output[\"l1\"][:, 0 : attention_mask.sum(), :].mean(dim = 1)\n",
        "  # again, we can choose to use differnt outputs from the BERT Model as the embeddings of the sentence/review\n",
        "  # here, we use the mean value of non-padded tokens from the last self-attention layer of BERT  \n",
        "  \n",
        "  return text_embedding\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP8fiJvA3NDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wPppoAwU6UY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "a43c75fa-e05f-4470-b481-68dcced1c649"
      },
      "source": [
        "label_review = new_raw[['text','label']]\n",
        "label_review"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This Columbia jacket is a pretty good quality ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Just as expected! Perfect rain jacket for todd...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I bought this for a Birthday gift and haven't ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This is perfect for those fall and spring days...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>good color</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135795</th>\n",
              "      <td>UPDATE 11/23/2018: I'm dropping the rating a s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135796</th>\n",
              "      <td>PaintShop Pro 2019 comes in a box with a DVD, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135797</th>\n",
              "      <td>I see by the reviews that a LOT of folks had p...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135798</th>\n",
              "      <td>I had the disc and still experienced installat...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135799</th>\n",
              "      <td>I hate this software. It is not user friendly....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>135800 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  label\n",
              "0       This Columbia jacket is a pretty good quality ...      1\n",
              "1       Just as expected! Perfect rain jacket for todd...      0\n",
              "2       I bought this for a Birthday gift and haven't ...      0\n",
              "3       This is perfect for those fall and spring days...      0\n",
              "4                                              good color      0\n",
              "...                                                   ...    ...\n",
              "135795  UPDATE 11/23/2018: I'm dropping the rating a s...      1\n",
              "135796  PaintShop Pro 2019 comes in a box with a DVD, ...      1\n",
              "135797  I see by the reviews that a LOT of folks had p...      0\n",
              "135798  I had the disc and still experienced installat...      1\n",
              "135799  I hate this software. It is not user friendly....      0\n",
              "\n",
              "[135800 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGFM-wq2KWYM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "5bc11d53-caa6-4944-92a1-8d1ddd5396f1"
      },
      "source": [
        "# test the function, focusing on the output of its shape\n",
        "text_to_embedding(32, label_review, 128).shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqv54jZzNs56",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "04e320c5-da13-4723-8323-7b0fc0033f1f"
      },
      "source": [
        "# use a loop to embed a number of reviews in the raw dataset\n",
        "# note that you can filter specific groups of obs in the raw dataset for further analysis\n",
        "# for instance, I only use positive reviews in the sample here.\n",
        "# later, I can find clusters in ONLY positive reviews, which could help me summarize \n",
        "# the main factors that make consumers give a positive review\n",
        "review_embeddings = []\n",
        "with torch.no_grad():\n",
        "  for index in range(2000):\n",
        "    review_embedding = text_to_embedding(index, label_review.loc[label_review['label'] == 1,:], 128)\n",
        "    review_embeddings.append(review_embedding)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkB3ULW6UmCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transfer the list of sentence embedings into a numpy array\n",
        "positive_embeddings = torch.cat(review_embeddings, 0)\n",
        "\n",
        "positive_embeddings = positive_embeddings.to('cpu')\n",
        "positive_embeddings = positive_embeddings.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQYoRmsI25SH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c88dc571-719e-421b-b4fa-cebb61f62029"
      },
      "source": [
        "type(positive_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s6TrXetan87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# do the cluster analysis\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsuRnkXvVA7-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "outputId": "378b83a5-a9d0-4dd2-9887-2f89e594d5bd"
      },
      "source": [
        "# use the dendrogram graph to identify the clusters\n",
        "import scipy.cluster.hierarchy as shc \n",
        "plt.figure(figsize = (100, 50))\n",
        "plt.title(\"review embeddings\")\n",
        "dend = shc.dendrogram(shc.linkage(positive_embeddings, method = \"ward\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 7200x3600 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imJJ0k7bLTSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "cluster = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
        "cluster_id = cluster.fit_predict(positive_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RJK8HwufQM4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "3bbc9e4f-761f-427a-eda3-95d40d372204"
      },
      "source": [
        "sample_review = label_review.loc[label_review['label'] == 1,:].iloc[0:2000 ,:]\n",
        "sample_review['cluster_id'] = cluster_id\n",
        "sample_review"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>cluster_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This Columbia jacket is a pretty good quality ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Love Columbia jackets. However, the logo is pe...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>This jacket fits perfectly. Size 2T. It has th...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>We like Columbia rain jackets because they fol...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>I like Columbia products because they are alwa...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111278</th>\n",
              "      <td>I had to contact Tech to finish installation a...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111286</th>\n",
              "      <td>The Norton 360 Deluxe - antivirus software was...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111298</th>\n",
              "      <td>First, installation is a bit of a pain, only b...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111300</th>\n",
              "      <td>Norton products are great protection for all d...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111301</th>\n",
              "      <td>This purchase turned into a nightmare. I have ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  label  cluster_id\n",
              "0       This Columbia jacket is a pretty good quality ...      1           1\n",
              "27      Love Columbia jackets. However, the logo is pe...      1           1\n",
              "56      This jacket fits perfectly. Size 2T. It has th...      1           1\n",
              "101     We like Columbia rain jackets because they fol...      1           1\n",
              "149     I like Columbia products because they are alwa...      1           1\n",
              "...                                                   ...    ...         ...\n",
              "111278  I had to contact Tech to finish installation a...      1           0\n",
              "111286  The Norton 360 Deluxe - antivirus software was...      1           1\n",
              "111298  First, installation is a bit of a pain, only b...      1           0\n",
              "111300  Norton products are great protection for all d...      1           1\n",
              "111301  This purchase turned into a nightmare. I have ...      1           0\n",
              "\n",
              "[2000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isdaRbDrfjrE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e6c83dbb-2b8e-4c92-bfc3-97188845d6b1"
      },
      "source": [
        "sample = sample_review.groupby('cluster_id').apply(pd.DataFrame.sample, 3).iloc[:,0]\n",
        "sample.iloc[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Love carhartt clothing. Extremely well made and rugged.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVvkadi93iCQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "2c27d200-15cb-4527-bdd5-d491018da2d7"
      },
      "source": [
        "sample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "cluster_id       \n",
              "0           13893    Usual Carhartt ruggedness--looks to be durable...\n",
              "            14294    Always trusted Carhartt and nothings changed. ...\n",
              "            14240    Love carhartt clothing. Extremely well made an...\n",
              "1           947      This is perfect for my son who likes to wear l...\n",
              "            11654    I bought a black pair for $12, definitely wort...\n",
              "            14137    I tried these due to some guys liking them. I'...\n",
              "2           9754     Love it. Would recommend to others. I got anot...\n",
              "            13936    Calhartt pants are my new pants since levi's d...\n",
              "            14004    I love carhartt but I bought these jeans over ...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSx5Saoz4WhK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2c8440fa-3a76-4233-c898-d93a821f14be"
      },
      "source": [
        "sample_review[sample_review['cluster_id']==2].sample()['text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11608    I used to buy these shorts at Target for use a...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NptVIU2O48ri",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "61b6cbee-633d-46ff-fe73-e646b9426132"
      },
      "source": [
        "new_raw.iloc[13893,1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Usual Carhartt ruggedness--looks to be durable with good stitchery.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81JsBi5f5CVh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "244201b6-4478-4d21-90d3-b34d7b457b54"
      },
      "source": [
        "new_raw.iloc[14004,1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I love carhartt but I bought these jeans over a month ago and they were too tight I sent them back in and I still have yet to get my refund it was received back in January'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUq3-AZI71a-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "c8f13944-4237-4625-f56a-d8b585469e01"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "pol = lambda x: TextBlob(x).sentiment.polarity\n",
        "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
        "\n",
        "sample_review['polarity'] = sample_review['text'].apply(pol)\n",
        "sample_review['subjectivity'] = sample_review['text'].apply(sub)\n",
        "sample_review.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>cluster_id</th>\n",
              "      <th>polarity</th>\n",
              "      <th>subjectivity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This Columbia jacket is a pretty good quality ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.347222</td>\n",
              "      <td>0.488889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Love Columbia jackets. However, the logo is pe...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>This jacket fits perfectly. Size 2T. It has th...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.518333</td>\n",
              "      <td>0.593333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>We like Columbia rain jackets because they fol...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.328889</td>\n",
              "      <td>0.497778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>I like Columbia products because they are alwa...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.233333</td>\n",
              "      <td>0.408333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  ...  subjectivity\n",
              "0    This Columbia jacket is a pretty good quality ...  ...      0.488889\n",
              "27   Love Columbia jackets. However, the logo is pe...  ...      0.300000\n",
              "56   This jacket fits perfectly. Size 2T. It has th...  ...      0.593333\n",
              "101  We like Columbia rain jackets because they fol...  ...      0.497778\n",
              "149  I like Columbia products because they are alwa...  ...      0.408333\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq3Bl5pP7-kV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "b2225506-5e3d-4903-c5bc-e3ebf07df2d0"
      },
      "source": [
        "sample_review.groupby('cluster_id').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>polarity</th>\n",
              "      <th>subjectivity</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cluster_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.170182</td>\n",
              "      <td>0.518238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.313040</td>\n",
              "      <td>0.513335</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            label  polarity  subjectivity\n",
              "cluster_id                               \n",
              "0               1  0.170182      0.518238\n",
              "1               1  0.313040      0.513335"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVceJlqziKgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rlu8dv4Kwa3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# things to be done\n",
        "# 1. universal sentence encodning from Google\n",
        "# 2. hierarchy clustering in python"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}