{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "label_test_template",
      "provenance": [],
      "collapsed_sections": [
        "B4yLzjne6Tm6",
        "NaSSY_mynUVw"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3df1b85f2475483fae84505071c658b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f93fdd94ab0249789e89fbc8e902d446",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8be862f805d84d02859c2f42b69a7640",
              "IPY_MODEL_b7d8ab59036a4cfd94ad3ecde9043dce"
            ]
          }
        },
        "f93fdd94ab0249789e89fbc8e902d446": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8be862f805d84d02859c2f42b69a7640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5cf5da5987ea4461be10c152e2de96c1",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_72f0320eb5e8463d84270efdc114441e"
          }
        },
        "b7d8ab59036a4cfd94ad3ecde9043dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_198b2ca9a15f4019bcc6971ca316abd5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 2.45kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_01681bcf2bea4081a1d5232d38c0dd25"
          }
        },
        "5cf5da5987ea4461be10c152e2de96c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "72f0320eb5e8463d84270efdc114441e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "198b2ca9a15f4019bcc6971ca316abd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "01681bcf2bea4081a1d5232d38c0dd25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d068ac7203ca4131a3963dc811c93938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f8e71284cb2a407aa9738ad03e39f766",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a2fde423d246434aa8f2dceff5d114eb",
              "IPY_MODEL_0008541488b1486a9e544fe6b4904838"
            ]
          }
        },
        "f8e71284cb2a407aa9738ad03e39f766": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a2fde423d246434aa8f2dceff5d114eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_039095ccbf234c0fa40aaaa731ac9a6e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6205b6b871bb4798a4e2a11498c92cee"
          }
        },
        "0008541488b1486a9e544fe6b4904838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4cc898b4107c46059dd322a13177b221",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:06&lt;00:00, 65.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d8aa503508e64d0ca36c61b08e47dc9f"
          }
        },
        "039095ccbf234c0fa40aaaa731ac9a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6205b6b871bb4798a4e2a11498c92cee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4cc898b4107c46059dd322a13177b221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d8aa503508e64d0ca36c61b08e47dc9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "592c4e827c8841308a5595536ba162ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_84c4c2280c20418eafe4a99ae0bbee9f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_87524de5a1554dc6bce3aa5eab87812d",
              "IPY_MODEL_72e74fb8ac7846d287aa52c04843dfa7"
            ]
          }
        },
        "84c4c2280c20418eafe4a99ae0bbee9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "87524de5a1554dc6bce3aa5eab87812d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5f612c42b9ce4a3aa2557226f3c2add1",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9adc7e222dd547ef9478e3b91f02d5b0"
          }
        },
        "72e74fb8ac7846d287aa52c04843dfa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5ed5dcbdda704bcfb362ea22712e8b36",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 871kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_21f3fd273c4646c6b3a9cdcaec991806"
          }
        },
        "5f612c42b9ce4a3aa2557226f3c2add1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9adc7e222dd547ef9478e3b91f02d5b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5ed5dcbdda704bcfb362ea22712e8b36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "21f3fd273c4646c6b3a9cdcaec991806": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-Bnqn00VGfY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "outputId": "234d3909-ffc0-4d7f-84bc-699f7589e167"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\r\u001b[K     |▍                               | 10kB 24.3MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 3.4MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 4.5MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 4.8MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 3.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 4.4MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 4.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81kB 5.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 92kB 5.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102kB 5.2MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 133kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████                          | 143kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 153kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 163kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 174kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 184kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 194kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 204kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 225kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 235kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 245kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 256kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 266kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 276kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 286kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 296kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 307kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 317kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 327kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 337kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 348kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 358kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 368kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 378kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 389kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 399kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 409kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 419kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 430kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 440kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 450kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 460kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 471kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 481kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 491kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 501kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 512kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 522kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 532kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 542kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 552kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 563kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 573kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 583kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 593kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 604kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 614kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 624kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 634kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 645kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 655kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 665kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 675kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 686kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 696kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 706kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 716kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 727kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 737kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 747kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 757kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 768kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 778kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 25.0MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 34.4MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 42.0MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 30.5MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 22.0MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61kB 24.9MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 27.2MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 28.3MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92kB 28.3MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 23.5MB/s eta 0:00:01\r\u001b[K     |███▍                            | 112kB 23.5MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122kB 23.5MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 23.5MB/s eta 0:00:01\r\u001b[K     |████▎                           | 143kB 23.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 153kB 23.5MB/s eta 0:00:01\r\u001b[K     |████▉                           | 163kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 174kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 184kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 194kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 204kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 215kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 225kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 235kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 245kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 256kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 266kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 276kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 286kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 296kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 307kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 317kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 327kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 337kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 348kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 358kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 368kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 378kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 389kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 399kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 409kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 419kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 430kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 440kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 450kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 460kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 471kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 481kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 491kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 501kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 512kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 522kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 532kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 542kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 552kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 563kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 573kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 583kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 593kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 604kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 614kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 624kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 634kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 645kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 655kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 665kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 675kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 686kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 696kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 706kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 716kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 727kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 737kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 747kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 757kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 768kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 778kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 788kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 798kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 808kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 819kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 829kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 839kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 849kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 860kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 870kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 880kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 890kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 901kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 911kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 921kB 23.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 931kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 942kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 952kB 23.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 962kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 972kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 983kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 993kB 23.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 64kB/s eta 0:00:02\r\u001b[K     |██████████████████████████████▏ | 1.0MB 64kB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.0MB 64kB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 64kB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 64kB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1MB 64kB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 64kB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 64kB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 23.8MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 45.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=f431e2c835ba3820558e2fe16fafb7f114fcc87ecf6e2b3f07ea32560e0d0f76\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67970XzEVCAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import transformers\n",
        "# per the setting of transformers, to use any of its NLP model\n",
        "# we need to have three things: that is BertTokenizer, BertModel, BertConfig\n",
        "from transformers import BertTokenizer, BertModel, BertConfig"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4socdVzVEHS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea7eca23-8dd7-47b0-990a-238e4797fcff"
      },
      "source": [
        "# first, let's see if we have GPU so that we could train our model in GPU\n",
        "# GPU is really at parallel computation\n",
        "from torch import cuda\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print(device)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59RK1lkXU9NH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "a403f806-b39b-43b9-df8e-fe2e4ff7f3ed"
      },
      "source": [
        "# collect Google Drive to Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2L80RjhU9s_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114,
          "referenced_widgets": [
            "3df1b85f2475483fae84505071c658b4",
            "f93fdd94ab0249789e89fbc8e902d446",
            "8be862f805d84d02859c2f42b69a7640",
            "b7d8ab59036a4cfd94ad3ecde9043dce",
            "5cf5da5987ea4461be10c152e2de96c1",
            "72f0320eb5e8463d84270efdc114441e",
            "198b2ca9a15f4019bcc6971ca316abd5",
            "01681bcf2bea4081a1d5232d38c0dd25",
            "d068ac7203ca4131a3963dc811c93938",
            "f8e71284cb2a407aa9738ad03e39f766",
            "a2fde423d246434aa8f2dceff5d114eb",
            "0008541488b1486a9e544fe6b4904838",
            "039095ccbf234c0fa40aaaa731ac9a6e",
            "6205b6b871bb4798a4e2a11498c92cee",
            "4cc898b4107c46059dd322a13177b221",
            "d8aa503508e64d0ca36c61b08e47dc9f"
          ]
        },
        "outputId": "cd9752ff-7441-4f75-cb30-44f70057dc69"
      },
      "source": [
        "# create an instance of BERT model \n",
        "# note that it takes time to download the BERT model (~ 440M)\n",
        "# BERT model is big, because it has a lot of paramters. \n",
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3df1b85f2475483fae84505071c658b4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d068ac7203ca4131a3963dc811c93938",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKo8Z3XUYKJ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "592c4e827c8841308a5595536ba162ba",
            "84c4c2280c20418eafe4a99ae0bbee9f",
            "87524de5a1554dc6bce3aa5eab87812d",
            "72e74fb8ac7846d287aa52c04843dfa7",
            "5f612c42b9ce4a3aa2557226f3c2add1",
            "9adc7e222dd547ef9478e3b91f02d5b0",
            "5ed5dcbdda704bcfb362ea22712e8b36",
            "21f3fd273c4646c6b3a9cdcaec991806"
          ]
        },
        "outputId": "aa48aae2-9db1-4a3d-805e-ce83ebed1c19"
      },
      "source": [
        "# creat an instance of BERT tokenizer\n",
        "# as you can tell, the tokenizer is pretty small, only 232k in size\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "592c4e827c8841308a5595536ba162ba",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyhRDyuDUGwu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 959
        },
        "outputId": "9382d664-4c68-4328-e700-c2f07cdbe0c5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = 'stage_1_label_sample.csv'\n",
        "raw_review = pd.read_csv(url)\n",
        "raw_review"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>product_title</th>\n",
              "      <th>product_category</th>\n",
              "      <th>star_rating</th>\n",
              "      <th>helpful_votes</th>\n",
              "      <th>total_votes</th>\n",
              "      <th>vine</th>\n",
              "      <th>verified_purchase</th>\n",
              "      <th>review_headline</th>\n",
              "      <th>review_body</th>\n",
              "      <th>review_date</th>\n",
              "      <th>social connectedness</th>\n",
              "      <th>environment</th>\n",
              "      <th>self_sufficiency</th>\n",
              "      <th>transparency_authenticity</th>\n",
              "      <th>tradition</th>\n",
              "      <th>individuality</th>\n",
              "      <th>diversity_equality</th>\n",
              "      <th>privacy</th>\n",
              "      <th>status</th>\n",
              "      <th>thrift_value</th>\n",
              "      <th>innovation</th>\n",
              "      <th>fun_adventure</th>\n",
              "      <th>health</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Crane Usa EE-5301 Cool Mist 2.3-Gallon Humidifier</td>\n",
              "      <td>Health &amp; Personal Care</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>If only it still worked....</td>\n",
              "      <td>This product was great! for about a month, I l...</td>\n",
              "      <td>7/28/2012</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Medline Herringbone Cotton Dish Towels, 1 Dozen</td>\n",
              "      <td>Health &amp; Personal Care</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Three Stars</td>\n",
              "      <td>The material isn't great. Doesn't dry hardly a...</td>\n",
              "      <td>12/16/2014</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CytoSport Muscle Milk Snack Protein Bar</td>\n",
              "      <td>Health &amp; Personal Care</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>good</td>\n",
              "      <td>This product tastes good, they are a bit small...</td>\n",
              "      <td>4/24/2013</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Toothco for ProResults</td>\n",
              "      <td>Health &amp; Personal Care</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Excellent</td>\n",
              "      <td>Excellent</td>\n",
              "      <td>8/6/2015</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reynolds Consumer Products 1001090008031 Alumi...</td>\n",
              "      <td>Health &amp; Personal Care</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Worst foil ever...</td>\n",
              "      <td>This foil is so thin that if a mosquito were t...</td>\n",
              "      <td>11/7/2013</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1656</th>\n",
              "      <td>Rocky Men's Thermal 2pc Set Long John Underwear…</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Comfortable fit</td>\n",
              "      <td>Its a value for money purchase which basically...</td>\n",
              "      <td>11/24/2014</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1657</th>\n",
              "      <td>Style J Indigo Elegance Long Denim Skirt</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Great skirt</td>\n",
              "      <td>I really like this skirt. The front seam is a ...</td>\n",
              "      <td>3/9/2015</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1658</th>\n",
              "      <td>Next Level Apparel Women's Terry Racerback Tank</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Not worth it</td>\n",
              "      <td>Stiff fabric - not stretchy Tshirt material - ...</td>\n",
              "      <td>9/4/2014</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1659</th>\n",
              "      <td>Playtex Women's Side Smoothing Underwire Bra</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Fits well, kind funky fabric</td>\n",
              "      <td>This bra fits well enough, definitely true to ...</td>\n",
              "      <td>9/18/2013</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1660</th>\n",
              "      <td>Women's Combed Cotton Rib-Knit Thermal Vest</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Women's Combed Cotton Rib-Knit Thermal Vest</td>\n",
              "      <td>it is not like it looks in the image!&lt;br /&gt;i d...</td>\n",
              "      <td>11/23/2012</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1661 rows × 23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          product_title  ... health\n",
              "0     Crane Usa EE-5301 Cool Mist 2.3-Gallon Humidifier  ...      0\n",
              "1       Medline Herringbone Cotton Dish Towels, 1 Dozen  ...      0\n",
              "2               CytoSport Muscle Milk Snack Protein Bar  ...      0\n",
              "3                                Toothco for ProResults  ...      0\n",
              "4     Reynolds Consumer Products 1001090008031 Alumi...  ...      0\n",
              "...                                                 ...  ...    ...\n",
              "1656   Rocky Men's Thermal 2pc Set Long John Underwear…  ...      0\n",
              "1657           Style J Indigo Elegance Long Denim Skirt  ...      0\n",
              "1658    Next Level Apparel Women's Terry Racerback Tank  ...      0\n",
              "1659       Playtex Women's Side Smoothing Underwire Bra  ...      0\n",
              "1660        Women's Combed Cotton Rib-Knit Thermal Vest  ...      0\n",
              "\n",
              "[1661 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVLbEIyuWoWW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "b3283988-77f6-473a-a085-6e130245125a"
      },
      "source": [
        "raw_review.iloc[398,:]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "product_title                Lite Source LS-21665BLK Desk Lamp with Black P...\n",
              "product_category                                              Home Improvement\n",
              "star_rating                                                                  1\n",
              "helpful_votes                                                                2\n",
              "total_votes                                                                  2\n",
              "vine                                                                         N\n",
              "verified_purchase                                                            N\n",
              "review_headline                                           GARBAGE!!!!!!!!!!!!!\n",
              "review_body                  Garbage!!!!!!!!!!!!!!!!!!!!! Ive had these for...\n",
              "review_date                                                          1/28/2014\n",
              "social connectedness                                                         0\n",
              "environment                                                                  0\n",
              "self_sufficiency                                                             0\n",
              "transparency_authenticity                                                    0\n",
              "tradition                                                                    0\n",
              "individuality                                                                0\n",
              "diversity_equality                                                           0\n",
              "privacy                                                                      0\n",
              "status                                                                       0\n",
              "thrift_value                                                                 1\n",
              "innovation                                                                   0\n",
              "fun_adventure                                                                0\n",
              "health                                                                       0\n",
              "Name: 398, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bO_VL6_GV6gF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_review = pd.DataFrame(None)\n",
        "\n",
        "train_review[['text','label']] = raw_review[['review_body','thrift_value']].iloc[0:1661,:]\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4F0PknWZqxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_review = train_review.dropna()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajQpmwt2Xfds",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "386fbbfd-af34-4c01-8b14-0b98412cb18c"
      },
      "source": [
        "train_review.describe()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1660.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.798795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.401022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             label\n",
              "count  1660.000000\n",
              "mean      0.798795\n",
              "std       0.401022\n",
              "min       0.000000\n",
              "25%       1.000000\n",
              "50%       1.000000\n",
              "75%       1.000000\n",
              "max       1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "513Gaudcu56u",
        "colab_type": "text"
      },
      "source": [
        "# Define a Customized Dataset Class and Setup the Dataloader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "luKm7Mh6vZi4",
        "colab": {}
      },
      "source": [
        "# first let's define some key parameters we will use later\n",
        "# note this is a relatively large number, making the training process slow, \n",
        "# but it's necessary becuase a lot of reviews are long.\n",
        "max_length = 128\n",
        "# how many raw inputs we feed into the train and validation model at once\n",
        "train_batch = 32\n",
        "valid_batch = 32\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "APMXoM-b81B4",
        "colab": {}
      },
      "source": [
        "# then we need to import the libraries we need\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5zOBRz6rvwfh",
        "colab": {}
      },
      "source": [
        "class YelpDataset(Dataset):\n",
        "\n",
        "  # here we want to  create a customized dataset--YelpDataset--which could take a raw text as input\n",
        "  # and encode it in BERT's way with a tokenizer. \n",
        "  # The tokenized input will be then fed into a BERT Model in the NN, which we will create later\n",
        "\n",
        "  # __init__ defines some necessary attributes for any instance created\n",
        "  # like: the dataset with raw text reviews, what tokenizer we want to use for encoding, \n",
        "  # the max length of sentence we need to pad or trancate\n",
        "  def __init__(self, dataset, tokenizer, max_len):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = dataset\n",
        "    self.text = dataset.text\n",
        "    self.label = dataset.label\n",
        "    self.max_len = max_len\n",
        "  \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # __getitem__ take index as input, \n",
        "    # in general/without customizatin, it returns the sample with the index in the dataset\n",
        "    # further we could customize its functionality, letting it to apply some operations on the \n",
        "    # index-specified sample before return a value for us\n",
        "    # here we use index to locate a specific review text we want to pre-process\n",
        "    # text = str(self.text[index])\n",
        "    # text = \" \".join(text.split())\n",
        "    text = self.text.iloc[index]\n",
        "\n",
        "    # then, we put the text into a BERT encode_plus\n",
        "    # important debug tips: \n",
        "    # for encode_plus, to keep all the raw inputs in the same lenght\n",
        "    # we need to specify BOTH padding and trancation in addition to max_length\n",
        "    # the former for short sentences and the latter for long ones\n",
        "    # failing to do either one will result in uneven lengths of encoded inputs,\n",
        "    # which will create troubles for your dataloader in the nn training sessions\n",
        "    outputs = self.tokenizer.encode_plus(\n",
        "        text,\n",
        "        max_length = self.max_len,\n",
        "        padding = \"max_length\",\n",
        "        truncation = \"longest_first\",\n",
        "        # note that in some versions of transformer in you local machine, the code is \n",
        "        # pad_to_max_length = True,\n",
        "        # truncation_strategy = 'longest_first',\n",
        "        # we might need to change the argument name a little to fit different version of transformers\n",
        "        add_special_tokens = True,\n",
        "        return_tensors = 'pt'\n",
        "    )\n",
        "    # recall the BERT Tokenizer session, it takes raw text as input\n",
        "    # and return input_ids, attention_mask\n",
        "    input_ids = outputs['input_ids']\n",
        "    attention_mask = outputs['attention_mask']\n",
        "    # we then store those values and put them together with the label info in the sample\n",
        "    # as the return of the __getitem__ method\n",
        "    return {\n",
        "        'input_ids': torch.tensor(input_ids, dtype = torch.long),\n",
        "        'attention_mask': torch.tensor(attention_mask, dtype = torch.long),\n",
        "        'label': torch.tensor(self.label.iloc[index], dtype = torch.float) \n",
        "    }\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tZLP1R7vBuKL",
        "colab": {}
      },
      "source": [
        "# split the train and validate dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_raw, valid_raw = train_test_split(train_review, test_size = 0.1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eRqVwGM5v2_d",
        "colab": {}
      },
      "source": [
        "# create instances of the YelpDataset for raw trianing and validate datasets\n",
        "# recall that tokenizer has be defined by: tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') in the BERT tokenizer session\n",
        "train_processed = YelpDataset(train_raw, tokenizer, max_length)\n",
        "valid_processed = YelpDataset(valid_raw, tokenizer, max_length)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eEl2Jkdksm4i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f8a095c-6183-49a9-d60d-8ddc275f5884"
      },
      "source": [
        "# check the attributes and method of train_processed\n",
        "train_processed.__len__()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1494"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s7HaY0bQCTXw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "68d6e55c-79c9-4f92-f89c-62c470be4bf2"
      },
      "source": [
        "# test the customized Dataset instance\n",
        "print(train_processed.__getitem__(10))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[ 101, 2573, 2986,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]]), 'label': tensor(1.)}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hvnlzcFRRcxw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "7b279e9b-af0b-494a-b0e2-1fccdf8631d2"
      },
      "source": [
        "# see the dimension of the pre-processed data\n",
        "print(train_processed.__getitem__(10)['input_ids'].shape)\n",
        "# we can use the squeeze() method to remove the axis of \"1\", a method we will use later\n",
        "print(train_processed.__getitem__(10)['input_ids'].squeeze().shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 128])\n",
            "torch.Size([128])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k4T7kvxJEpZX",
        "colab": {}
      },
      "source": [
        "# important, run it again to create a new dataset loader for every NN model you train\n",
        "# lastly, set up the dataloader that serves as a pipeline feeding pre-processed data into the neural network\n",
        "train_sampler = RandomSampler(train_processed)\n",
        "train_loader = DataLoader(train_processed, batch_size = train_batch, num_workers = 0)\n",
        "valid_sampler = SequentialSampler(valid_processed)\n",
        "valid_loader = DataLoader(valid_processed, batch_size = valid_batch, num_workers = 0)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gl3urqqxHeH9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "fd5a30e3-0591-4087-b48d-b2efdfe8272e"
      },
      "source": [
        "# exploer the attributes of dataloder\n",
        "# length of dataloader = len(dataset)/batch size\n",
        "print(len(train_loader))\n",
        "print(len(valid_loader))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4yLzjne6Tm6",
        "colab_type": "text"
      },
      "source": [
        "# Define a Customized Neural Network with the First Hidden Layer as NLP Encoding Layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hLuI0Q3ZF2C2"
      },
      "source": [
        "Up to now, we have defined the customized Dataset class that preprocess the raw input, encoding them into input_ids and attention_mask that BERT model needs. We also set up the Dataloader that feed the pre-processed data into the Neural Network we are creating now. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oGetmmKNafv5",
        "colab": {}
      },
      "source": [
        "# import the functions we need use in the Netwrok Model\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ld8Oqz5Jv43c",
        "colab": {}
      },
      "source": [
        "# the neural network we create is a class called  YelpBERT, which inherits\n",
        "# the attributes and structures of the torch.nn.Module\n",
        "\n",
        "# the __init__ functino defines the necessary hidden layers of the NN\n",
        "# and the forward function set up the computation graph: the real calculation procedures of the NN\n",
        "# see the class session 7's slides and recording for details \n",
        "\n",
        "class YelpBERT(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(YelpBERT, self).__init__()\n",
        "    # recall that, we have defined model as: model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    # see BERT Model's input and output: https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "    self.l1 = model\n",
        "\n",
        "    # with this layer a percentage of all neurons will be randomly turned-off to prevent overfitting\n",
        "    # since our training dataset might be small, we really cannot afford a large neural network\n",
        "    # the output of the BERT layer is a vector of 768 elements, if fully connected to next linear layer\n",
        "    # then the current layer would have 768 neurons, too much! \n",
        "    # using dropout mechanism, we can randomly turn off a percentage of the neuraon in the training process\n",
        "    # literally reduced the number of neuron in the layer\n",
        "    self.l2 = torch.nn.Dropout(0.3) \n",
        "\n",
        "    # if you want to have multiple fully connected linear layer, use this\n",
        "    # self.l2 = torch.nn.Linear(768, 10)\n",
        "    # but again, if your training dataset is not very large, we may only offard one linear layer\n",
        "    \n",
        "    # last layer\n",
        "    self.l3 = torch.nn.Linear(768, 1)\n",
        "\n",
        "\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    # first layer\n",
        "    # the first layer utilize the BERT model (call \"model\" in __init__) to transfer the input_ids into\n",
        "    # contextualized word embeddings --numerical vectors\n",
        "    # we can customize the output of this layer such as using the [CLS] token or the mean of all input tokens\n",
        "   \n",
        "    # if you want to use the BERT output of the last self-attention layer, use this:\n",
        "    last, pooler = self.l1(input_ids = input_ids, attention_mask = attention_mask)\n",
        "   \n",
        "    # if you want to use BERT output from OTHER self-attention layers,use this:\n",
        "    # (note that BERT base model has 12 hidden self-attention layers), \n",
        "    # last, pooler, all = self.l1(input_ids = input_ids, attention_mask = attention_mask, output_hidden_states = True)\n",
        "    \n",
        "    # second layer\n",
        "    # here we could use the mean value of tokens of all raw inputs as the embedding of the whole input text\n",
        "    # and feed it into the second layer, use this: \n",
        "    # output from last BERT self-attention layer:\n",
        "    # initial index \"0\" for mean value of [cls] token and all non-padded tokens\n",
        "    # initial index \"1\" for mean value of all non-padded tokens only\n",
        "    output = last[:, 0 : attention_mask.sum(), :].mean(dim = 1)\n",
        "    # BERT output from other hidden self-attention layer:\n",
        "    # output = all[11][:, 0 : attention_mask.sum(), :].mean(dim = 1)\n",
        "    \n",
        "    # or use the [CLS] token of last layer\n",
        "    # output = last[:, 0, :]\n",
        "\n",
        "    # output from BERT model now be fed into a relu activation funcation adn\n",
        "    # the second layer of the Neural Network\n",
        "    output = F.relu(output.squeeze())\n",
        "    output = F.relu(self.l2(output))\n",
        "\n",
        "    # why squeeze(): to make the dimension of input-output across layers consistent\n",
        "    # e.g., the output of layer 1--self.l1, is in the shape of (1, 768),\n",
        "    # we use .squeeze() to make it in a shape of (768) only, \n",
        "    # because the later layer--l2 adn l2--take (768) as input dimension not (1, 768)\n",
        "\n",
        "    # third layer\n",
        "    output = self.l3(output)\n",
        "\n",
        "    # last sigmoid layer to furhter transfer the single scalar of l3 into a probability\n",
        "    return torch.sigmoid(output)\n",
        "    # note that we can also customize the layers after the 1st one, \n",
        "    # making more layers (i.e., a deeper NN) and see if it perform betters "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m8yf6W_iSdCJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "7a5a0530-dafb-4ef5-dc20-4e9b77eadbe9"
      },
      "source": [
        "# test the model step-by-step\n",
        "# compare the output dimension to your expectation\n",
        "# https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "# testing each layer in the NN is very important\n",
        "# we should pay close attention the the dimensions of the inputs and outputs of each layer \n",
        "# use .squeeze() to remove uncessary axis whose length is 1 to make the dimensions consistent. like \"output = pooler.squeeze()\" in the NN above\n",
        "l1 = model\n",
        "l1.to(device)\n",
        "input_ids = train_processed.__getitem__(10)['input_ids'].to(device)\n",
        "attention_mask = train_processed.__getitem__(10)['attention_mask'].to(device)\n",
        "last, pooler = l1(input_ids = input_ids, attention_mask = attention_mask)\n",
        "print(last.shape)\n",
        "print(last.squeeze().shape)\n",
        "print(pooler.shape)\n",
        "print(pooler.squeeze().shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 128, 768])\n",
            "torch.Size([128, 768])\n",
            "torch.Size([1, 768])\n",
            "torch.Size([768])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zuR1CGxHpNDt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "907f9ce2-0dcf-4bc5-bf92-b31f5057ba69"
      },
      "source": [
        "# similarly, test if the dimensions of input and output \n",
        "output = last[:, 1 : attention_mask.sum(), :].mean(dim = 1)\n",
        "output = output.squeeze()\n",
        "output.shape\n",
        "l2 = torch.nn.Dropout(0.5)   \n",
        "l2.to(device)\n",
        "l3 = torch.nn.Linear(768, 1)\n",
        "l3.to(device)\n",
        "output.to(device)\n",
        "l3(l2(torch.tensor(output)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.4857], device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NaSSY_mynUVw"
      },
      "source": [
        "# Create a BERT + NN Model as an Instance of the Customized Model Classs Defined Above and Setup the Loss Function and Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aFDDnyD6wlD4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "44415042-1b7d-4835-d61e-7afe7c966e84"
      },
      "source": [
        "# create an instance of the YelpBERT model\n",
        "# remember to recreat a instance of the Model Class after you modified the YelpBERT class\n",
        "model_yelp = YelpBERT()\n",
        "model_yelp.to(device)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "YelpBERT(\n",
              "  (l1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (l2): Dropout(p=0.3, inplace=False)\n",
              "  (l3): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2BnImCNBLxG_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0de51261-e1be-4d63-8d2e-4d3204eddc31"
      },
      "source": [
        "# test the model_yelp\n",
        "# recall that the train_processed is a special class that could pre-process the raw input words into input_ids\n",
        "# and attentino_mask using its method __getitem__ method\n",
        "# we then use this method to get the input_ids and attention_mask that we need to feed in the YelpBERT() model\n",
        "\n",
        "test_output = model_yelp(input_ids, attention_mask)\n",
        "print(len(test_output))\n",
        "print(test_output)\n",
        "# note that the output is a scalar, becasue ofthe last hidden layer in the NN, self.l3 = torch.nn.Linear(768, 1)\n",
        "# the scalar then can be put into a softmax for prediction \n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "tensor([0.4245], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7QmZRjGkPVAI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d9083e18-1314-4b2a-e2ac-5de18baeced3"
      },
      "source": [
        "print(input_ids.shape)\n",
        "print(attention_mask.shape)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 128])\n",
            "torch.Size([1, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nrrzk6ukepKO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a73dfe7a-3eb3-4d97-acbb-7da6f09770cb"
      },
      "source": [
        "# check out the model parameters\n",
        "# note that the first BERT layer, the word embeddings are in the shape of (30522, 768)\n",
        "# and we can tell that the first 12 layers of our model are normal BERT layers, \n",
        "# the last two are what we customized-- a dropout layer and the linear layer\n",
        "model_yelp.parameters"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of YelpBERT(\n",
              "  (l1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (l2): Dropout(p=0.3, inplace=False)\n",
              "  (l3): Linear(in_features=768, out_features=1, bias=True)\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4enOR93aXnCG"
      },
      "source": [
        "Up to now, we have the Dataset (pre-process/encode the raw text inputs, making them into input_ids that BERT model takes as input), the Dataloader (feed the processed data into the NN in batchs), the neural network (with the first layer as a BERT model, the transfer the encoded input_ids into word embeddings, and later layers just work on those embeddings/numerical vectors as normal neural network). \n",
        "\n",
        "And finally, we could set up our training loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iZpo8C-zc-B3"
      },
      "source": [
        "First we start with: the loss function (e.g., crossentropy loss or mean squared error loss) and the optimize schedule (some thing about learning rate, adaptive learning rate, see class 7)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7TRC6Dbzc8oZ",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q4QtC037dzxO",
        "colab": {}
      },
      "source": [
        "# criterion is the loss function we use\n",
        "# here we use Binary CrossEntropy Loss, you can try others see the performance difference\n",
        "# https://pytorch.org/docs/master/generated/torch.nn.BCELoss.html\n",
        "criterion = nn.BCELoss()\n",
        "# note that loss function in pytorch framework usually take the pair of (prediction, ground_truth) as input\n",
        "# and give the loss value as output"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d0MvxRbleYiO",
        "colab": {}
      },
      "source": [
        "# optimizer is our optimize strategy, here we use stachastic gradient descending as the approch \n",
        "# to update our model parameters. \n",
        "# tips: previously I trained multiple models but with learning rate = 1e-05, 3e-05, \n",
        "# after 4 epochs the model performance almost doesn't change\n",
        "# thus now I use 10e-05, it seems the performance improves faster\n",
        "learning_rate = 10e-05\n",
        "# SGD is a common optimizer, but let's use Adam here, AdamW is a optimizer developed by Huggingface using Adam's mechanism\n",
        "# optimizer = optim.SGD(model_yelp.parameters(), lr = learning_rate, momentum = 0.9)\n",
        "# http://deeplearning.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/\n",
        "# you can try out other optimization method and see performance difference"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "njBahjNyjByc",
        "colab": {}
      },
      "source": [
        "# set the epoch\n",
        "epochs = 5\n",
        "# epochs means how many rounds each training sample will be fed into the NN.\n",
        "# next, we need to supply our model \"model_yelp\" to the GPU, so it can be run on GPU\n",
        "# model_yelp.to(device)\n",
        "# since the GPU has a lot of cores, it takes some time to supply to model to the GPU\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEUQBFhMhsHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try another optimizer\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "optimizer = AdamW(model_yelp.parameters(),lr = learning_rate,eps = 1e-8)\n",
        "                  \n",
        "total_steps = len(train_loader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler that update the learning rate gradully, this scheduler is with AdamW\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WUy2sgBBgz50",
        "colab": {}
      },
      "source": [
        "# lastly, let's define a helper function we can use for calculating the prediction accuracy\n",
        "import numpy as np\n",
        "# the idea of the funtion is that a vector of predicted probablity of being good review, \n",
        "# which is the output of the sigmoid/last layer of the neural network, is compared with the true label\n",
        "# is the predicted probability >= 0.5, we assign 1, otherwise we assign 0, we use np.around() achieve this\n",
        "def pred_accuracy (prediction, label):\n",
        "  pred_flat = np.around(prediction).flatten()\n",
        "  label_flat = label.flatten()\n",
        "  return np.sum(pred_flat == label_flat) / len(label_flat)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2WFSgu6_1VL",
        "colab_type": "text"
      },
      "source": [
        "# Train the BERT + NN Model and Evaluate the Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DFPc5CRqiESW",
        "colab": {}
      },
      "source": [
        "# set the random set the same, making the results reproducible\n",
        "import random\n",
        "seed_val = 45\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_SCqZW_Dn6x5"
      },
      "source": [
        "OK, finally, we strat to train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpYKGk-NeCUZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "19056860-18c9-41f7-c565-8b8bbafc44f2"
      },
      "source": [
        "# create an instance of the YelpBERT model\n",
        "model_yelp = YelpBERT()\n",
        "model_yelp.to(device)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "YelpBERT(\n",
              "  (l1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (l2): Dropout(p=0.3, inplace=False)\n",
              "  (l3): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a417dCiz3VWS",
        "colab": {}
      },
      "source": [
        "# set up the directory for storing the trained model\n",
        "import os\n",
        "# save to Google Drive\n",
        "dir = \"/content/drive/My Drive\"\n",
        "# save to local file\n",
        "# dir = \"E://OneDrive - lmu.edu//Python Projects//BERT and ML\"\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H0gMiqrmix5g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "c3f41e82-4188-4312-81ad-91127af27ace"
      },
      "source": [
        "# Very Important: Each Time When You Train a Neural Network Again, \n",
        "# Plase recreate the nural network, the dataloader, and the optimizer and its scheduler.\n",
        "\n",
        "\n",
        "# train the model \n",
        "# store the total loss and accuracy values of each epoch\n",
        "training_stats = []\n",
        "\n",
        "# the epoch loop, number of rounds specified by epochs\n",
        "for epoch_i in range(0, epochs):\n",
        "  print('======== Epoch {:} / {:} =========='.format(epoch_i + 1, epochs))\n",
        "  # zero the values of total loss and accuracy of the epoch\n",
        "  total_loss = 0\n",
        "  total_accuracy = 0\n",
        "  # in the training stage, set the model into train mode\n",
        "  model_yelp.train()\n",
        "\n",
        "  # the training step loop\n",
        "  # recall that train_loader feed data into the NN in batchs to save memory and improve efficiency\n",
        "  # then the # of total steps is about (# of samples/batch size)\n",
        "  print(\"training\")\n",
        "  for step, batch in enumerate(train_loader, 0):\n",
        "    # feed the data into GPU using .to(device) method\n",
        "    # note that the input_ids for one raw input is in the shape of (1, max_length)\n",
        "    # after be processed into batch, the input_ids become (batch_size, 1, max_length)\n",
        "    # however, BERT only takes input_ids in the shape of (batch_size, max_length),\n",
        "    # (see here: https://huggingface.co/transformers/model_doc/bert.html#bertmodel)\n",
        "    # so we need to do \"batch['input_ids'].squeeze()\" inestead of \"batch['input_ids']\"\n",
        "    # to remove the unnecessy axis of \"1\". \n",
        "    # same operation for attention_mask\n",
        "    input_ids = batch['input_ids'].squeeze().to(device, dtype = torch.long)\n",
        "    attention_mask = batch['attention_mask'].squeeze().to(device, dtype = torch.long)\n",
        "    label = batch['label'].to(device, dtype = torch.float)\n",
        "\n",
        "    # at each step, before the NN does the feed forward, let's set the gradient to 0\n",
        "    # as pytorch nn.Module automatically cumulates gradient from previous rounds\n",
        "    # this is good for RNN training, but not necessy for us here. Thus, we turn it off.   \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # for each raw input, the feed forward calculation give us two scalar, \n",
        "    # representing the score/probability of the sample being 0/bad review class or 1/good review class  \n",
        "    # note that since we feed the inputs/raw samples in batch, \n",
        "    # the prediciton should be in the shape of (batch_size, # of classes)\n",
        "    prediction = model_yelp(input_ids, attention_mask)\n",
        "    prediction = prediction.squeeze()\n",
        "\n",
        "    # criterion is defined as a cross entropy loss function\n",
        "    # it takes (prediction, ground truth) as input arugments\n",
        "    # the former should be in the shape of (batch_size, # of classes), \n",
        "    # the latter should be in the sahpe of (batch_szie, 1), \"1\" dimension records the true class id of the input\n",
        "    # https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html   \n",
        "    loss = criterion(prediction, label)\n",
        "\n",
        "    # with the loss, we can do back propagation to calculate the gradient\n",
        "    # very easy, just one line of code  \n",
        "    loss.backward()\n",
        "\n",
        "    # with the gradient, we can update the model paramters. \n",
        "    # recall how we define the optimizer in the above cell  \n",
        "    optimizer.step()\n",
        "    \n",
        "    # update the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # to keep tracking on the model performance, we accumulate the total loss in every epoch  \n",
        "    total_loss += loss.item()\n",
        "      \n",
        "    # for every 10 steps, we print out the epoce # and loss\n",
        "    # again, the max_length of raw text input is 256, relatively long than usual, thus it will take longer to train. \n",
        "    if step%10 == 0 and step != 0:\n",
        "      print(f'Epoch:{epoch_i + 1}, Total_Loss:{total_loss}, Average_Loss:{total_loss/step}')\n",
        "  \n",
        "  # calcualte and store the average training loss of each batch in the current epoch\n",
        "  avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "  \n",
        "\n",
        "  # now let's set up the validation loop, meaning the trained model above will be used to evaluate the sample in validation dataset\n",
        "  # note that this validation loop is in the same \"indent\" level as the training loop, and they both under the epoches loop\n",
        "  print(\"validating\")\n",
        "  # set the model now in the evaluation mode\n",
        "  model_yelp.eval()\n",
        "  # zero the values of total loss and accuracy\n",
        "  total_loss = 0\n",
        "  #total_accuracy = 0\n",
        "\n",
        "  # validation loop\n",
        "  for step, batch in enumerate(valid_loader, 0):\n",
        "    input_ids = batch['input_ids'].squeeze().to(device, dtype = torch.long)\n",
        "    attention_mask = batch['attention_mask'].squeeze().to(device, dtype = torch.long)\n",
        "    label = batch['label'].to(device, dtype = torch.float)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction = model_yelp(input_ids, attention_mask)\n",
        "        prediction = prediction.squeeze()\n",
        "        loss = criterion(prediction, label)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    if step%10 == 0 and step != 0:\n",
        "      print(f'Epoch:{epoch_i + 1}, Total_Loss:{total_loss}, Average_Loss:{total_loss/step}')\n",
        "\n",
        "  # calcualte and store the average training loss of each batch in the current epoch\n",
        "  avg_valid_loss = total_loss / len(valid_loader)\n",
        "\n",
        "  training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_valid_loss\n",
        "        }\n",
        "    )\n",
        "  \n",
        "\n",
        "\n",
        "  # lastly, at the end of each epoch, let's save the model for later use\n",
        "  # note that dir is defined before as \"./content/drive/My Drive\"  \n",
        "  torch.save(model_yelp.state_dict(), os.path.join(dir, 'exper-epoch-{}.pt'.format(epoch_i)))\n",
        "\n",
        "print(\"training complete!\")\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======== Epoch 1 / 5 ==========\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Total_Loss:5.771788537502289, Average_Loss:0.5771788537502289\n",
            "Epoch:1, Total_Loss:10.556575268507004, Average_Loss:0.5278287634253502\n",
            "Epoch:1, Total_Loss:14.797370702028275, Average_Loss:0.49324569006760915\n",
            "Epoch:1, Total_Loss:18.405294239521027, Average_Loss:0.46013235598802565\n",
            "validating\n",
            "======== Epoch 2 / 5 ==========\n",
            "training\n",
            "Epoch:2, Total_Loss:4.308077305555344, Average_Loss:0.43080773055553434\n",
            "Epoch:2, Total_Loss:8.507423371076584, Average_Loss:0.42537116855382917\n",
            "Epoch:2, Total_Loss:11.587778806686401, Average_Loss:0.38625929355621336\n",
            "Epoch:2, Total_Loss:13.87386679649353, Average_Loss:0.34684666991233826\n",
            "validating\n",
            "======== Epoch 3 / 5 ==========\n",
            "training\n",
            "Epoch:3, Total_Loss:1.987084500491619, Average_Loss:0.1987084500491619\n",
            "Epoch:3, Total_Loss:5.1954105123877525, Average_Loss:0.2597705256193876\n",
            "Epoch:3, Total_Loss:6.827588550746441, Average_Loss:0.22758628502488137\n",
            "Epoch:3, Total_Loss:8.288080293685198, Average_Loss:0.20720200734212996\n",
            "validating\n",
            "======== Epoch 4 / 5 ==========\n",
            "training\n",
            "Epoch:4, Total_Loss:1.499357920140028, Average_Loss:0.1499357920140028\n",
            "Epoch:4, Total_Loss:3.129243765026331, Average_Loss:0.15646218825131655\n",
            "Epoch:4, Total_Loss:4.100113183259964, Average_Loss:0.1366704394419988\n",
            "Epoch:4, Total_Loss:5.133652564138174, Average_Loss:0.12834131410345434\n",
            "validating\n",
            "======== Epoch 5 / 5 ==========\n",
            "training\n",
            "Epoch:5, Total_Loss:0.6949077341705561, Average_Loss:0.0694907734170556\n",
            "Epoch:5, Total_Loss:1.8192871287465096, Average_Loss:0.09096435643732548\n",
            "Epoch:5, Total_Loss:2.3033184949308634, Average_Loss:0.07677728316436211\n",
            "Epoch:5, Total_Loss:2.834513096138835, Average_Loss:0.07086282740347087\n",
            "validating\n",
            "training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B5soA_VPd2Ry",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "outputId": "d6407e8a-09ec-44e1-c1a4-96b61e98c772"
      },
      "source": [
        "# plot the train statistics stored in training_stats\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set(style = \"darkgrid\")\n",
        "\n",
        "sns.set(font_scale = 1.5)\n",
        "plt.rcParams['figure.figsize'] = [12, 6]\n",
        "\n",
        "df_stats = pd.DataFrame(data = training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f35f2281668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9zXX5Pdvw9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make sure to save the model performance statistics and the associated model configuration. \n",
        "# that is save the df_stats to csv file with a file name, say experiment 1\n",
        "df_stats.to_csv(os.path.join(dir, 'label_experiment_3.csv'))\n",
        "# save the key hyper-parameters of this training experiment, say experiment 1\n",
        "label_experiment_3_config = {\n",
        "    \"epochs\": epochs,\n",
        "    \"train_batch_size\" : train_batch,\n",
        "    \"valid_batch_size\" : valid_batch,\n",
        "    \"initial_learning_rate\": learning_rate,\n",
        "     \"max_sentence_length\": max_length,\n",
        "     \"loss_fucntion\": criterion,\n",
        "     \"optimizer\": optimizer,\n",
        "     # you need to manually type-in the following info\n",
        "     \"BERT output\": \"mean value of [cls] embeddings of non-padded token from the second to the last layer\",\n",
        "     \"activation function\": \"relu\",\n",
        "     \"dropout rate of BERT output\": model_yelp.l2,\n",
        "     \"# of fully connected linear layer\": 1,\n",
        "     \"dataset\": \"Yelp Review Balanced\",\n",
        "     \"comment\": \"sees not as good as the outputs of last layer\"\n",
        "}\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odvnYYhghsH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the experiment configurate assocaited with this experiment\n",
        "# note that if you click the file icon (the third vertical one on the far left)\n",
        "# you will see the save files, double click on them, you can see them.\n",
        "import csv\n",
        "with open(os.path.join(dir, 'label_experiment_3_config.csv'), 'w') as csv_file:  \n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in label_experiment_3_config.items():\n",
        "       writer.writerow([key, value])"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WinIsU6zMpWt",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate the Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p9MQQ6yU9kLo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "837d1d56-0243-4436-af78-9f913c9cb4a2"
      },
      "source": [
        "# apply the trained model to the validation dataset\n",
        "# get the model predictions and compare the comparisons to the true labels\n",
        "model_yelp.eval()\n",
        "predictions, labels = [], []\n",
        "for step, batch in enumerate(valid_loader):\n",
        "  input_ids = batch['input_ids'].squeeze().to(device, dtype = torch.long)\n",
        "  attention_mask = batch['attention_mask'].squeeze().to(device, dtype = torch.long)\n",
        "  label = batch['label'].to('cpu').numpy()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    prediction = model_yelp(input_ids, attention_mask)\n",
        "\n",
        "  prediction = prediction.detach().cpu().numpy()\n",
        "  predictions.append(prediction)\n",
        "  labels.append(label)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tA38tbL6Qkv7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e07b370f-aea4-430b-e70e-95f381a4d32a"
      },
      "source": [
        "# call the helper function-- pred_accuacy to compute the prediction accuracy in each batch\n",
        "ac = []\n",
        "for i in range(len(predictions)):\n",
        "  ac_i = pred_accuracy(predictions[i], labels[i])\n",
        "  ac.append(ac_i)\n",
        "ac"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.71875, 0.6875, 0.8125, 0.8125, 0.875, 0.3333333333333333]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QaL7PHMQOvjZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "6da554ef-e423-4787-dc08-4a795d0941f1"
      },
      "source": [
        "# transfer the outcomes into np\n",
        "predictions = np.asarray(predictions)\n",
        "labels = np.asarray(labels)\n",
        "predictions[0]\n",
        "# note that now the outcomes are still stored in batches"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9837781 ],\n",
              "       [0.98990065],\n",
              "       [0.9888254 ],\n",
              "       [0.30090493],\n",
              "       [0.98892516],\n",
              "       [0.9828243 ],\n",
              "       [0.9903002 ],\n",
              "       [0.9300906 ],\n",
              "       [0.9896852 ],\n",
              "       [0.98907834],\n",
              "       [0.5227331 ],\n",
              "       [0.9897461 ],\n",
              "       [0.32645935],\n",
              "       [0.02015004],\n",
              "       [0.9746329 ],\n",
              "       [0.9884926 ],\n",
              "       [0.17403996],\n",
              "       [0.9888398 ],\n",
              "       [0.9893351 ],\n",
              "       [0.01812033],\n",
              "       [0.30276194],\n",
              "       [0.98932225],\n",
              "       [0.9892352 ],\n",
              "       [0.9889498 ],\n",
              "       [0.9895132 ],\n",
              "       [0.09094027],\n",
              "       [0.98963845],\n",
              "       [0.9895923 ],\n",
              "       [0.98907393],\n",
              "       [0.98962575],\n",
              "       [0.9887242 ],\n",
              "       [0.98877794]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWiqZYkAhsID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c3ad04d-3b24-45be-8d72-94bbec412755"
      },
      "source": [
        "# convert predictions stored in the batches into a long vector\n",
        "pred = np.concatenate(predictions, axis=0 )\n",
        "pred = np.concatenate(pred, axis=0 )\n",
        "pred = pred.reshape(len(pred),1)\n",
        "print(pred.shape)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(166, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5kkRWD0hsIE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a37cae35-87cd-471d-ec42-5e800924053d"
      },
      "source": [
        "# convert the true labels batches into a long vector\n",
        "true_label = np.concatenate(labels, axis=0 )\n",
        "true_label = true_label.reshape(len(true_label), 1)\n",
        "print(true_label.shape)\n",
        "type(true_label)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(166, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXEasNQvhsIF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "f4eaaa9c-d8a0-4129-9810-ea8b13af999e"
      },
      "source": [
        "# put the predictions and labels into the same dataset\n",
        "df = np.concatenate([pred, true_label], axis = 1)\n",
        "df = pd.DataFrame(data=df, columns=[\"preds\", \"labels\"])\n",
        "df"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.983778</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.989901</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.988825</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.300905</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.988925</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>0.989512</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>0.989254</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>0.896469</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>0.979321</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>0.988310</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>166 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        preds  labels\n",
              "0    0.983778     1.0\n",
              "1    0.989901     1.0\n",
              "2    0.988825     1.0\n",
              "3    0.300905     1.0\n",
              "4    0.988925     1.0\n",
              "..        ...     ...\n",
              "161  0.989512     1.0\n",
              "162  0.989254     1.0\n",
              "163  0.896469     0.0\n",
              "164  0.979321     0.0\n",
              "165  0.988310     0.0\n",
              "\n",
              "[166 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pjqBh76onyd-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b8650d9-f7bd-41a7-a04e-197dc55f6d95"
      },
      "source": [
        "# see the total prediction accuracy\n",
        "sum((df[\"preds\"]>=0.5) == df[\"labels\"])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "127"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r7T9I_yhsIJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "70ca0db7-071b-40e1-b84d-0ee34c8c86d8"
      },
      "source": [
        "# find the index of the review that has the lowest predicted probabilty(of being a positive review) in true_label == 1 group. \n",
        "df.loc[df.loc[df['labels'] == 1, :].idxmin()]"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.018120</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.983778</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       preds  labels\n",
              "19  0.018120     1.0\n",
              "0   0.983778     1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VurVKLthsIK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1f1f1c37-f678-481f-b295-2ac823f78e0c"
      },
      "source": [
        "# see that review\n",
        "valid_raw.iloc[19,0]\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"My daughter is 18 lbs and only 8 months old this was to small she can't even wear it..it's cute but there's a problem with the sizes.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9PDrrPNhsIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "b30624b0-431a-4ddb-be29-ad61565e589a"
      },
      "source": [
        "# alternatively, for all the reviews that have true_label == 1, \n",
        "# let's sort their predicted probabilities\n",
        "df.loc[df['labels'] == 1, :].sort_values('preds')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.018120</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>0.018314</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0.110095</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.127225</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0.139268</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>0.990068</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.990083</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>0.990091</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>0.990116</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.990300</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>123 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        preds  labels\n",
              "19   0.018120     1.0\n",
              "156  0.018314     1.0\n",
              "50   0.110095     1.0\n",
              "95   0.127225     1.0\n",
              "79   0.139268     1.0\n",
              "..        ...     ...\n",
              "62   0.990068     1.0\n",
              "32   0.990083     1.0\n",
              "59   0.990091     1.0\n",
              "133  0.990116     1.0\n",
              "6    0.990300     1.0\n",
              "\n",
              "[123 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FZooOSNTGDl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "0fde5f4a-2687-4a38-8328-8baa02555bbb"
      },
      "source": [
        "df[df['labels']==1].preds.sort_values()[0:20]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19     0.018120\n",
              "156    0.018314\n",
              "50     0.110095\n",
              "95     0.127225\n",
              "79     0.139268\n",
              "16     0.174040\n",
              "45     0.187827\n",
              "135    0.190562\n",
              "39     0.234544\n",
              "3      0.300905\n",
              "12     0.326459\n",
              "35     0.338936\n",
              "10     0.522733\n",
              "98     0.523693\n",
              "148    0.562095\n",
              "158    0.683918\n",
              "116    0.841305\n",
              "145    0.914988\n",
              "7      0.930091\n",
              "64     0.934018\n",
              "Name: preds, dtype: float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wslamubJhsIQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ca1c59cb-f8e2-4b5a-9867-ca087fea9e55"
      },
      "source": [
        "# see the reviews\n",
        "valid_raw.iloc[23,0]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"Having purchased a smaller unit Eaton several years ago, I was prepared to be unimpressed. But this slightly larger unit has far superior tuning, reception and sound quality. I was disappointed to see that it doesn't come with a wall adapter, but later realized that it's not necessary because the crank is so incredibly effective. A brisk two-minute crank literally powered the radio for over five hours! It's so easy there really seems to me to be little reason to install batteries in storage and watch them slowly die.<br /><br />It should be noted that the manual indicates a limit on the total number of manual charges the internal battery can sustain, which means cranking may not be a suitable technique for routine operation! But as a standby communication device it seems just about perfect.<br /><br />I'm quite impressed with the value contained in this nice little package.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otzAxck1hsIR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "75b1c60d-5ff3-4870-db8e-b369274eaf5c"
      },
      "source": [
        "# on the other way around, for all the reviews whose label == 0, \n",
        "# let's sort their predicted probablities in descending order\n",
        "df[df['labels'] == 0].sort_values('preds', ascending = False)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>0.989721</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.989638</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>0.989274</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.988724</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>0.988724</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.988605</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.988493</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>0.988310</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>0.985793</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0.984287</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>0.984204</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>0.983429</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.982824</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>0.981063</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.980475</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>0.979321</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>0.979229</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.974633</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>0.971148</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.938675</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>0.914328</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>0.896469</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>0.844945</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>0.680684</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>0.658556</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>0.623580</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>0.565244</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>0.475626</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>0.450548</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.302762</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>0.163414</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.090940</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.051409</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.045824</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.028779</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.020150</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>0.019463</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>0.018204</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>0.016540</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>0.015060</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>0.014687</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>0.013861</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.013593</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        preds  labels\n",
              "126  0.989721     0.0\n",
              "26   0.989638     0.0\n",
              "65   0.989274     0.0\n",
              "30   0.988724     0.0\n",
              "81   0.988724     0.0\n",
              "43   0.988605     0.0\n",
              "15   0.988493     0.0\n",
              "165  0.988310     0.0\n",
              "119  0.985793     0.0\n",
              "78   0.984287     0.0\n",
              "121  0.984204     0.0\n",
              "159  0.983429     0.0\n",
              "5    0.982824     0.0\n",
              "52   0.981063     0.0\n",
              "44   0.980475     0.0\n",
              "164  0.979321     0.0\n",
              "84   0.979229     0.0\n",
              "14   0.974633     0.0\n",
              "131  0.971148     0.0\n",
              "49   0.938675     0.0\n",
              "51   0.914328     0.0\n",
              "163  0.896469     0.0\n",
              "63   0.844945     0.0\n",
              "160  0.680684     0.0\n",
              "111  0.658556     0.0\n",
              "118  0.623580     0.0\n",
              "106  0.565244     0.0\n",
              "149  0.475626     0.0\n",
              "75   0.450548     0.0\n",
              "20   0.302762     0.0\n",
              "132  0.163414     0.0\n",
              "25   0.090940     0.0\n",
              "37   0.051409     0.0\n",
              "40   0.045824     0.0\n",
              "72   0.028779     0.0\n",
              "13   0.020150     0.0\n",
              "137  0.019463     0.0\n",
              "139  0.018204     0.0\n",
              "103  0.016540     0.0\n",
              "92   0.015060     0.0\n",
              "140  0.014687     0.0\n",
              "68   0.013861     0.0\n",
              "93   0.013593     0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjDkJr_VhsIU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e52542ac-d78b-4f84-bd5f-5cad53ff0085"
      },
      "source": [
        "# see the reviews\n",
        "# after learning some examples, it seems that our model will give a high score as long as the food is good\n",
        "# even though the service is not. \n",
        "valid_raw.iloc[26,0]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"This little radio does what it's advertised to do. Digital AM/FM tuner in a rugged package. No bells or whistles that you don't need anyway. Battery compartment has a rubber gasket to keep water out. No Complaints!\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    }
  ]
}